{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Clayton Cohn<br>\n",
    "May 20, 2020<br>\n",
    "DSC 478<br>\n",
    "Prof. Mobasher"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assignment 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "DATA_PATH = \"/Users/claytoncohn/Dropbox/New/DePaul/DSC478/data/segmentation_data/\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For this assignment you will experiment with Principal Component Analysis as a dimensionality reduction approach to assist in clustering high-dimensional data. \n",
    "\n",
    "You will also experiment with item-based recommendation for a joke recommender system."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__1. PCA for Reduced Dimensionality in Clustering [Dataset: segmentation_data.zip]__\n",
    "\n",
    "For this problem you will use an image segmentation data set for clustering. \n",
    "\n",
    "You will experiment with using PCA as an approach to reduce dimensionality and noise in the data. \n",
    "\n",
    "You will compare the results of clustering the data with and without PCA using the provided image class assignments as the ground truth. \n",
    "\n",
    "The data set is divided into three files. The file \"segmentation_data.txt\" contains data about images with each line corresponding to one image. \n",
    "\n",
    "Each image is represented by 19 features (these are the columns in the data and correspond to the feature names in the file \"segmentation_names.txt\". \n",
    "\n",
    "The file \"segmentation_classes.txt\" contains the class labels (the type of image) and a numeric class label for each of the corresponding images in the data file. \n",
    "\n",
    "After clustering the image data, you will use the class labels to measure completeness and homogeneity of the generated clusters. \n",
    "\n",
    "The data set used in this problem is based on the Image Segmentation data set at the UCI Machine Learning Repository."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    a. Load in the image data matrix (with rows as images and columns as features). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>10</th>\n",
       "      <th>11</th>\n",
       "      <th>12</th>\n",
       "      <th>13</th>\n",
       "      <th>14</th>\n",
       "      <th>15</th>\n",
       "      <th>16</th>\n",
       "      <th>17</th>\n",
       "      <th>18</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>110.0</td>\n",
       "      <td>189.0</td>\n",
       "      <td>9</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>1.222222</td>\n",
       "      <td>1.186342</td>\n",
       "      <td>12.925926</td>\n",
       "      <td>10.888889</td>\n",
       "      <td>9.222222</td>\n",
       "      <td>18.666668</td>\n",
       "      <td>-6.111111</td>\n",
       "      <td>-11.111111</td>\n",
       "      <td>17.222221</td>\n",
       "      <td>18.666668</td>\n",
       "      <td>0.508139</td>\n",
       "      <td>1.910864</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>86.0</td>\n",
       "      <td>187.0</td>\n",
       "      <td>9</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.111111</td>\n",
       "      <td>0.720082</td>\n",
       "      <td>1.444444</td>\n",
       "      <td>0.750309</td>\n",
       "      <td>13.740741</td>\n",
       "      <td>11.666667</td>\n",
       "      <td>10.333334</td>\n",
       "      <td>19.222221</td>\n",
       "      <td>-6.222222</td>\n",
       "      <td>-10.222222</td>\n",
       "      <td>16.444445</td>\n",
       "      <td>19.222221</td>\n",
       "      <td>0.463329</td>\n",
       "      <td>1.941465</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>225.0</td>\n",
       "      <td>244.0</td>\n",
       "      <td>9</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.388889</td>\n",
       "      <td>2.195113</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>1.520234</td>\n",
       "      <td>12.259259</td>\n",
       "      <td>10.333334</td>\n",
       "      <td>9.333334</td>\n",
       "      <td>17.111110</td>\n",
       "      <td>-5.777778</td>\n",
       "      <td>-8.777778</td>\n",
       "      <td>14.555555</td>\n",
       "      <td>17.111110</td>\n",
       "      <td>0.480149</td>\n",
       "      <td>1.987902</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>47.0</td>\n",
       "      <td>232.0</td>\n",
       "      <td>9</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.277778</td>\n",
       "      <td>1.254621</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.894427</td>\n",
       "      <td>12.703704</td>\n",
       "      <td>11.000000</td>\n",
       "      <td>9.000000</td>\n",
       "      <td>18.111110</td>\n",
       "      <td>-5.111111</td>\n",
       "      <td>-11.111111</td>\n",
       "      <td>16.222221</td>\n",
       "      <td>18.111110</td>\n",
       "      <td>0.500966</td>\n",
       "      <td>1.875362</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>97.0</td>\n",
       "      <td>186.0</td>\n",
       "      <td>9</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.166667</td>\n",
       "      <td>0.691215</td>\n",
       "      <td>1.166667</td>\n",
       "      <td>1.005540</td>\n",
       "      <td>15.592592</td>\n",
       "      <td>13.888889</td>\n",
       "      <td>11.777778</td>\n",
       "      <td>21.111110</td>\n",
       "      <td>-5.111111</td>\n",
       "      <td>-11.444445</td>\n",
       "      <td>16.555555</td>\n",
       "      <td>21.111110</td>\n",
       "      <td>0.442661</td>\n",
       "      <td>1.863654</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>157.0</td>\n",
       "      <td>221.0</td>\n",
       "      <td>9</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.055556</td>\n",
       "      <td>0.646930</td>\n",
       "      <td>1.222222</td>\n",
       "      <td>0.620633</td>\n",
       "      <td>12.111111</td>\n",
       "      <td>10.222222</td>\n",
       "      <td>8.111112</td>\n",
       "      <td>18.000000</td>\n",
       "      <td>-5.666666</td>\n",
       "      <td>-12.000000</td>\n",
       "      <td>17.666666</td>\n",
       "      <td>18.000000</td>\n",
       "      <td>0.549180</td>\n",
       "      <td>1.877146</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>62.0</td>\n",
       "      <td>224.0</td>\n",
       "      <td>9</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.944445</td>\n",
       "      <td>1.083547</td>\n",
       "      <td>2.333333</td>\n",
       "      <td>1.632993</td>\n",
       "      <td>14.629630</td>\n",
       "      <td>13.222222</td>\n",
       "      <td>11.444445</td>\n",
       "      <td>19.222221</td>\n",
       "      <td>-4.222222</td>\n",
       "      <td>-9.555555</td>\n",
       "      <td>13.777778</td>\n",
       "      <td>19.222221</td>\n",
       "      <td>0.408965</td>\n",
       "      <td>1.860191</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>27.0</td>\n",
       "      <td>248.0</td>\n",
       "      <td>9</td>\n",
       "      <td>0.111111</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.611111</td>\n",
       "      <td>0.646930</td>\n",
       "      <td>3.166667</td>\n",
       "      <td>1.722401</td>\n",
       "      <td>15.296296</td>\n",
       "      <td>14.777778</td>\n",
       "      <td>12.888889</td>\n",
       "      <td>18.222221</td>\n",
       "      <td>-1.555556</td>\n",
       "      <td>-7.222222</td>\n",
       "      <td>8.777778</td>\n",
       "      <td>18.222221</td>\n",
       "      <td>0.312227</td>\n",
       "      <td>1.783512</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>44.0</td>\n",
       "      <td>233.0</td>\n",
       "      <td>9</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.222222</td>\n",
       "      <td>2.146487</td>\n",
       "      <td>2.111111</td>\n",
       "      <td>1.327766</td>\n",
       "      <td>14.481482</td>\n",
       "      <td>12.555555</td>\n",
       "      <td>11.333333</td>\n",
       "      <td>19.555555</td>\n",
       "      <td>-5.777778</td>\n",
       "      <td>-9.444445</td>\n",
       "      <td>15.222222</td>\n",
       "      <td>19.555555</td>\n",
       "      <td>0.422174</td>\n",
       "      <td>1.950405</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>17.0</td>\n",
       "      <td>229.0</td>\n",
       "      <td>9</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.111111</td>\n",
       "      <td>1.985130</td>\n",
       "      <td>2.444445</td>\n",
       "      <td>1.614747</td>\n",
       "      <td>13.703704</td>\n",
       "      <td>11.222222</td>\n",
       "      <td>11.111111</td>\n",
       "      <td>18.777779</td>\n",
       "      <td>-7.444445</td>\n",
       "      <td>-7.777778</td>\n",
       "      <td>15.222222</td>\n",
       "      <td>18.777779</td>\n",
       "      <td>0.439852</td>\n",
       "      <td>2.099904</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      0      1   2         3    4         5         6         7         8   \\\n",
       "0  110.0  189.0   9  0.000000  0.0  1.000000  0.666667  1.222222  1.186342   \n",
       "1   86.0  187.0   9  0.000000  0.0  1.111111  0.720082  1.444444  0.750309   \n",
       "2  225.0  244.0   9  0.000000  0.0  3.388889  2.195113  3.000000  1.520234   \n",
       "3   47.0  232.0   9  0.000000  0.0  1.277778  1.254621  1.000000  0.894427   \n",
       "4   97.0  186.0   9  0.000000  0.0  1.166667  0.691215  1.166667  1.005540   \n",
       "5  157.0  221.0   9  0.000000  0.0  1.055556  0.646930  1.222222  0.620633   \n",
       "6   62.0  224.0   9  0.000000  0.0  0.944445  1.083547  2.333333  1.632993   \n",
       "7   27.0  248.0   9  0.111111  0.0  1.611111  0.646930  3.166667  1.722401   \n",
       "8   44.0  233.0   9  0.000000  0.0  2.222222  2.146487  2.111111  1.327766   \n",
       "9   17.0  229.0   9  0.000000  0.0  2.111111  1.985130  2.444445  1.614747   \n",
       "\n",
       "          9          10         11         12        13         14         15  \\\n",
       "0  12.925926  10.888889   9.222222  18.666668 -6.111111 -11.111111  17.222221   \n",
       "1  13.740741  11.666667  10.333334  19.222221 -6.222222 -10.222222  16.444445   \n",
       "2  12.259259  10.333334   9.333334  17.111110 -5.777778  -8.777778  14.555555   \n",
       "3  12.703704  11.000000   9.000000  18.111110 -5.111111 -11.111111  16.222221   \n",
       "4  15.592592  13.888889  11.777778  21.111110 -5.111111 -11.444445  16.555555   \n",
       "5  12.111111  10.222222   8.111112  18.000000 -5.666666 -12.000000  17.666666   \n",
       "6  14.629630  13.222222  11.444445  19.222221 -4.222222  -9.555555  13.777778   \n",
       "7  15.296296  14.777778  12.888889  18.222221 -1.555556  -7.222222   8.777778   \n",
       "8  14.481482  12.555555  11.333333  19.555555 -5.777778  -9.444445  15.222222   \n",
       "9  13.703704  11.222222  11.111111  18.777779 -7.444445  -7.777778  15.222222   \n",
       "\n",
       "          16        17        18  \n",
       "0  18.666668  0.508139  1.910864  \n",
       "1  19.222221  0.463329  1.941465  \n",
       "2  17.111110  0.480149  1.987902  \n",
       "3  18.111110  0.500966  1.875362  \n",
       "4  21.111110  0.442661  1.863654  \n",
       "5  18.000000  0.549180  1.877146  \n",
       "6  19.222221  0.408965  1.860191  \n",
       "7  18.222221  0.312227  1.783512  \n",
       "8  19.555555  0.422174  1.950405  \n",
       "9  18.777779  0.439852  2.099904  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = pd.read_csv(DATA_PATH + \"segmentation_data.txt\", header=None)\n",
    "X.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    Also load in the numeric class labels from the segmentation class file. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    0\n",
       "1    0\n",
       "2    0\n",
       "3    0\n",
       "4    0\n",
       "5    0\n",
       "6    0\n",
       "7    0\n",
       "8    0\n",
       "9    0\n",
       "Name: 1, dtype: int64"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y = pd.read_csv(DATA_PATH + \"segmentation_classes.txt\", header=None, sep=\"\\t\").iloc[:, 1]\n",
    "y.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>REGION-CENTROID-COL</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>REGION-CENTROID-ROW</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>REGION-PIXEL-COUNT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>SHORT-LINE-DENSITY-5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>SHORT-LINE-DENSITY-2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                      0\n",
       "0   REGION-CENTROID-COL\n",
       "1   REGION-CENTROID-ROW\n",
       "2    REGION-PIXEL-COUNT\n",
       "3  SHORT-LINE-DENSITY-5\n",
       "4  SHORT-LINE-DENSITY-2"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "features = pd.read_csv(DATA_PATH + \"segmentation_names.txt\", header=None, sep=\"\\t\")\n",
    "features.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    Using your favorite method (e.g., sklearn's min-max scaler), perform min-max normalization on the data matrix\n",
    "    so that each feature is scaled to [0,1] range."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[4.30830040e-01, 7.41666667e-01, 0.00000000e+00, 0.00000000e+00,\n",
       "        0.00000000e+00, 3.42205474e-02, 6.72233922e-04, 2.73291926e-02,\n",
       "        8.55743510e-04, 9.01110284e-02, 7.94165331e-02, 6.11192912e-02,\n",
       "        1.30943107e-01, 7.31343290e-01, 1.41176540e-02, 8.72865267e-01,\n",
       "        1.23711348e-01, 5.08138840e-01, 8.31849232e-01],\n",
       "       [3.35968379e-01, 7.33333333e-01, 0.00000000e+00, 0.00000000e+00,\n",
       "        0.00000000e+00, 3.80228046e-02, 7.26095734e-04, 3.22981359e-02,\n",
       "        5.41219947e-04, 9.57913810e-02, 8.50891441e-02, 6.84830672e-02,\n",
       "        1.34840205e-01, 7.29477615e-01, 2.35294199e-02, 8.59582565e-01,\n",
       "        1.27393216e-01, 4.63329080e-01, 8.36986460e-01],\n",
       "       [8.85375494e-01, 9.70833333e-01, 0.00000000e+00, 0.00000000e+00,\n",
       "        0.00000000e+00, 1.15969577e-01, 2.21344355e-03, 6.70807367e-02,\n",
       "        1.09658970e-03, 8.54634659e-02, 7.53646732e-02, 6.18556741e-02,\n",
       "        1.20031165e-01, 7.36940304e-01, 3.88235327e-02, 8.27324481e-01,\n",
       "        1.13402054e-01, 4.80149030e-01, 8.44782328e-01],\n",
       "       [1.81818182e-01, 9.20833333e-01, 0.00000000e+00, 0.00000000e+00,\n",
       "        0.00000000e+00, 4.37262383e-02, 1.26509804e-03, 2.23602471e-02,\n",
       "        6.45176713e-04, 8.85618432e-02, 8.02269050e-02, 5.96465386e-02,\n",
       "        1.27045974e-01, 7.48134334e-01, 1.41176540e-02, 8.55787468e-01,\n",
       "        1.20029447e-01, 5.00965950e-01, 8.25889142e-01],\n",
       "       [3.79446640e-01, 7.29166667e-01, 0.00000000e+00, 0.00000000e+00,\n",
       "        0.00000000e+00, 3.99239709e-02, 6.96986866e-04, 2.60869646e-02,\n",
       "        7.25325858e-04, 1.08701264e-01, 1.01296598e-01, 7.80559656e-02,\n",
       "        1.48090401e-01, 7.48134334e-01, 1.05882352e-02, 8.61480079e-01,\n",
       "        1.39911626e-01, 4.42660570e-01, 8.23923576e-01]])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "scaler = MinMaxScaler(feature_range=(0,1))\n",
    "X_norm = scaler.fit_transform(X)\n",
    "X_norm[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    b. Using the Kmeans implementation in scikit-learn, perform clustering on the image data (use K = 7 in your \n",
    "    clustering so that later we can compare the clusters to the 7 pre-assigned image classes). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "KMeans(algorithm='auto', copy_x=True, init='k-means++', max_iter=1000,\n",
       "       n_clusters=7, n_init=10, n_jobs=None, precompute_distances='auto',\n",
       "       random_state=None, tol=0.0001, verbose=0)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#KMeans code taken from lecture notebook\n",
    "from sklearn.cluster import KMeans\n",
    "\n",
    "# Can only use Euclidean distance with sklearn KMeans\n",
    "kmeans = KMeans(n_clusters=7,max_iter=1000,verbose=0)\n",
    "kmeans.fit(X_norm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
       "       2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
       "       2, 2, 2, 2, 0, 5, 0, 0, 0, 0, 5, 5, 0, 0, 0, 0, 0, 0, 0, 5, 0, 0,\n",
       "       0, 0, 5, 0, 5, 0, 5, 0, 0, 0, 0, 0, 5, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 5, 0, 5, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 5, 0, 0, 0, 0, 0, 5, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 5, 0, 0, 0, 0, 5, 0, 5, 0, 0, 0, 5, 5,\n",
       "       5, 0, 5, 0, 0, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6,\n",
       "       6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 1, 6, 6, 6, 6, 6, 6, 6, 6, 6,\n",
       "       6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6,\n",
       "       6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6,\n",
       "       6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 5, 0, 0, 5, 0, 5, 5, 0, 6,\n",
       "       0, 5, 0, 5, 0, 5, 5, 5, 4, 5, 5, 0, 0, 5, 5, 5, 5, 0, 5, 0, 0, 5,\n",
       "       0, 0, 0, 5, 5, 5, 0, 4, 0, 5, 5, 5, 5, 5, 0, 0, 0, 0, 0, 4, 5, 5,\n",
       "       5, 4, 5, 0, 5, 0, 0, 5, 5, 5, 0, 5, 4, 0, 5, 5, 5, 5, 5, 5, 5, 5,\n",
       "       5, 0, 0, 0, 5, 0, 0, 5, 0, 4, 5, 5, 0, 5, 5, 5, 0, 5, 4, 0, 5, 5,\n",
       "       5, 5, 0, 5, 4, 5, 5, 5, 5, 5, 0, 5, 4, 5, 0, 0, 5, 0, 5, 5, 0, 0,\n",
       "       0, 5, 5, 0, 5, 4, 5, 5, 5, 5, 5, 5, 5, 5, 0, 5, 0, 5, 4, 0, 5, 0,\n",
       "       5, 5, 5, 0, 4, 0, 5, 4, 0, 0, 0, 0, 0, 5, 5, 5, 0, 5, 0, 5, 1, 4,\n",
       "       1, 4, 4, 1, 4, 6, 4, 1, 4, 4, 4, 1, 1, 1, 1, 4, 4, 1, 1, 4, 4, 1,\n",
       "       4, 4, 4, 4, 4, 1, 1, 4, 1, 4, 4, 1, 4, 1, 1, 1, 1, 4, 1, 4, 1, 5,\n",
       "       1, 1, 5, 1, 4, 4, 1, 1, 4, 5, 4, 1, 1, 1, 1, 1, 4, 1, 1, 1, 4, 1,\n",
       "       4, 1, 6, 4, 4, 1, 1, 4, 1, 4, 1, 4, 4, 1, 1, 1, 1, 1, 1, 1, 4, 4,\n",
       "       4, 4, 4, 1, 1, 1, 1, 4, 4, 1, 3, 3, 3, 3, 3, 3], dtype=int32)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clusters = kmeans.predict(X_norm)\n",
    "clusters[:500]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    Print the cluster centroids (use some formatting so that they are visually understandable)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>(REGION-CENTROID-COL,)</th>\n",
       "      <th>(REGION-CENTROID-ROW,)</th>\n",
       "      <th>(REGION-PIXEL-COUNT,)</th>\n",
       "      <th>(SHORT-LINE-DENSITY-5,)</th>\n",
       "      <th>(SHORT-LINE-DENSITY-2,)</th>\n",
       "      <th>(VEDGE-MEAN,)</th>\n",
       "      <th>(VEDGE-SD,)</th>\n",
       "      <th>(HEDGE-MEAN,)</th>\n",
       "      <th>(HEDGE-SD,)</th>\n",
       "      <th>(INTENSITY-MEAN,)</th>\n",
       "      <th>(RAWRED-MEAN,)</th>\n",
       "      <th>(RAWBLUE-MEAN,)</th>\n",
       "      <th>(RAWGREEN-MEAN,)</th>\n",
       "      <th>(EXRED-MEAN,)</th>\n",
       "      <th>(EXBLUE-MEAN,)</th>\n",
       "      <th>(EXGREEN-MEAN,)</th>\n",
       "      <th>(VALUE-MEAN,)</th>\n",
       "      <th>(SATURATION-MEAN,)</th>\n",
       "      <th>(HUE-MEAN,)</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.75</td>\n",
       "      <td>0.53</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.04</td>\n",
       "      <td>0.04</td>\n",
       "      <td>0.11</td>\n",
       "      <td>0.02</td>\n",
       "      <td>0.11</td>\n",
       "      <td>0.02</td>\n",
       "      <td>0.30</td>\n",
       "      <td>0.28</td>\n",
       "      <td>0.35</td>\n",
       "      <td>0.27</td>\n",
       "      <td>0.59</td>\n",
       "      <td>0.45</td>\n",
       "      <td>0.31</td>\n",
       "      <td>0.35</td>\n",
       "      <td>0.30</td>\n",
       "      <td>0.16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.25</td>\n",
       "      <td>0.46</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.03</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.04</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.03</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.03</td>\n",
       "      <td>0.02</td>\n",
       "      <td>0.04</td>\n",
       "      <td>0.02</td>\n",
       "      <td>0.77</td>\n",
       "      <td>0.22</td>\n",
       "      <td>0.51</td>\n",
       "      <td>0.04</td>\n",
       "      <td>0.80</td>\n",
       "      <td>0.18</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.51</td>\n",
       "      <td>0.81</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.08</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.05</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.05</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.11</td>\n",
       "      <td>0.09</td>\n",
       "      <td>0.09</td>\n",
       "      <td>0.14</td>\n",
       "      <td>0.68</td>\n",
       "      <td>0.08</td>\n",
       "      <td>0.82</td>\n",
       "      <td>0.13</td>\n",
       "      <td>0.41</td>\n",
       "      <td>0.89</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.54</td>\n",
       "      <td>0.15</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.03</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.03</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.03</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.82</td>\n",
       "      <td>0.78</td>\n",
       "      <td>0.89</td>\n",
       "      <td>0.79</td>\n",
       "      <td>0.27</td>\n",
       "      <td>0.67</td>\n",
       "      <td>0.29</td>\n",
       "      <td>0.89</td>\n",
       "      <td>0.21</td>\n",
       "      <td>0.13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.26</td>\n",
       "      <td>0.39</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.07</td>\n",
       "      <td>0.02</td>\n",
       "      <td>0.08</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.06</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.15</td>\n",
       "      <td>0.14</td>\n",
       "      <td>0.19</td>\n",
       "      <td>0.12</td>\n",
       "      <td>0.72</td>\n",
       "      <td>0.34</td>\n",
       "      <td>0.36</td>\n",
       "      <td>0.19</td>\n",
       "      <td>0.41</td>\n",
       "      <td>0.20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.30</td>\n",
       "      <td>0.53</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.05</td>\n",
       "      <td>0.05</td>\n",
       "      <td>0.10</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.08</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.40</td>\n",
       "      <td>0.37</td>\n",
       "      <td>0.47</td>\n",
       "      <td>0.35</td>\n",
       "      <td>0.50</td>\n",
       "      <td>0.57</td>\n",
       "      <td>0.21</td>\n",
       "      <td>0.47</td>\n",
       "      <td>0.30</td>\n",
       "      <td>0.16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.77</td>\n",
       "      <td>0.43</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.02</td>\n",
       "      <td>0.04</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.02</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.04</td>\n",
       "      <td>0.04</td>\n",
       "      <td>0.06</td>\n",
       "      <td>0.03</td>\n",
       "      <td>0.78</td>\n",
       "      <td>0.22</td>\n",
       "      <td>0.49</td>\n",
       "      <td>0.06</td>\n",
       "      <td>0.54</td>\n",
       "      <td>0.24</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   (REGION-CENTROID-COL,)  (REGION-CENTROID-ROW,)  (REGION-PIXEL-COUNT,)  \\\n",
       "0                    0.75                    0.53                   0.00   \n",
       "1                    0.25                    0.46                   0.00   \n",
       "2                    0.51                    0.81                   0.00   \n",
       "3                    0.54                    0.15                   0.00   \n",
       "4                    0.26                    0.39                   0.00   \n",
       "5                    0.30                    0.53                   0.00   \n",
       "6                    0.77                    0.43                   0.00   \n",
       "\n",
       "   (SHORT-LINE-DENSITY-5,)  (SHORT-LINE-DENSITY-2,)  (VEDGE-MEAN,)  \\\n",
       "0                     0.04                     0.04           0.11   \n",
       "1                     0.03                     0.01           0.04   \n",
       "2                     0.08                     0.01           0.05   \n",
       "3                     0.03                     0.00           0.03   \n",
       "4                     0.07                     0.02           0.08   \n",
       "5                     0.05                     0.05           0.10   \n",
       "6                     0.01                     0.02           0.04   \n",
       "\n",
       "   (VEDGE-SD,)  (HEDGE-MEAN,)  (HEDGE-SD,)  (INTENSITY-MEAN,)  (RAWRED-MEAN,)  \\\n",
       "0         0.02           0.11         0.02               0.30            0.28   \n",
       "1         0.00           0.03         0.00               0.03            0.02   \n",
       "2         0.00           0.05         0.00               0.11            0.09   \n",
       "3         0.00           0.03         0.00               0.82            0.78   \n",
       "4         0.00           0.06         0.00               0.15            0.14   \n",
       "5         0.01           0.08         0.01               0.40            0.37   \n",
       "6         0.00           0.02         0.00               0.04            0.04   \n",
       "\n",
       "   (RAWBLUE-MEAN,)  (RAWGREEN-MEAN,)  (EXRED-MEAN,)  (EXBLUE-MEAN,)  \\\n",
       "0             0.35              0.27           0.59            0.45   \n",
       "1             0.04              0.02           0.77            0.22   \n",
       "2             0.09              0.14           0.68            0.08   \n",
       "3             0.89              0.79           0.27            0.67   \n",
       "4             0.19              0.12           0.72            0.34   \n",
       "5             0.47              0.35           0.50            0.57   \n",
       "6             0.06              0.03           0.78            0.22   \n",
       "\n",
       "   (EXGREEN-MEAN,)  (VALUE-MEAN,)  (SATURATION-MEAN,)  (HUE-MEAN,)  \n",
       "0             0.31           0.35                0.30         0.16  \n",
       "1             0.51           0.04                0.80         0.18  \n",
       "2             0.82           0.13                0.41         0.89  \n",
       "3             0.29           0.89                0.21         0.13  \n",
       "4             0.36           0.19                0.41         0.20  \n",
       "5             0.21           0.47                0.30         0.16  \n",
       "6             0.49           0.06                0.54         0.24  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.options.display.float_format='{:,.2f}'.format\n",
    "centroids = pd.DataFrame(kmeans.cluster_centers_, columns=features)\n",
    "centroids"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    To evaluate your clusters, first perform Silhouette analysis on the clusters (compute Silhouette values for all \n",
    "    instances in the data, and then compute the overall mean Silhouette value; optionally, you can provide a \n",
    "    visaulization of the Silhouettes). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.57236961 0.5609926  0.46573747 0.49551578 0.57187855 0.56013527\n",
      " 0.54247867 0.41254281 0.51997981 0.46760414 0.48688779 0.49140278\n",
      " 0.58437635 0.56254708 0.3794524  0.5311416  0.54891047 0.4226305\n",
      " 0.4161636  0.40382238]\n"
     ]
    }
   ],
   "source": [
    "# Silhouette analysis code taken from lecture notebooks\n",
    "from sklearn import metrics\n",
    "\n",
    "silhouettes = metrics.silhouette_samples(X_norm, clusters)\n",
    "print(silhouettes[:20])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.3320776409159453\n"
     ]
    }
   ],
   "source": [
    "print(silhouettes.mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from matplotlib import cm\n",
    "from sklearn.metrics import silhouette_samples\n",
    "import pylab as pl\n",
    "\n",
    "def plot_silhouettes(data, clusters, metric='euclidean'):\n",
    "    \n",
    "    from matplotlib import cm\n",
    "    from sklearn.metrics import silhouette_samples\n",
    "\n",
    "    cluster_labels = np.unique(clusters)\n",
    "    n_clusters = cluster_labels.shape[0]\n",
    "    silhouette_vals = metrics.silhouette_samples(data, clusters, metric='euclidean')\n",
    "    c_ax_lower, c_ax_upper = 0, 0\n",
    "    cticks = []\n",
    "    for i, k in enumerate(cluster_labels):\n",
    "        c_silhouette_vals = silhouette_vals[clusters == k]\n",
    "        c_silhouette_vals.sort()\n",
    "        c_ax_upper += len(c_silhouette_vals)\n",
    "        color = cm.jet(float(i) / n_clusters)\n",
    "        pl.barh(range(c_ax_lower, c_ax_upper), c_silhouette_vals, height=1.0, \n",
    "                      edgecolor='none', color=color)\n",
    "\n",
    "        cticks.append((c_ax_lower + c_ax_upper) / 2)\n",
    "        c_ax_lower += len(c_silhouette_vals)\n",
    "    \n",
    "    silhouette_avg = np.mean(silhouette_vals)\n",
    "    pl.axvline(silhouette_avg, color=\"red\", linestyle=\"--\") \n",
    "\n",
    "    pl.yticks(cticks, cluster_labels)\n",
    "    pl.ylabel('Cluster')\n",
    "    pl.xlabel('Silhouette coefficient')\n",
    "\n",
    "    pl.tight_layout()\n",
    "    #pl.savefig('images/11_04.png', dpi=300)\n",
    "    pl.show()\n",
    "    \n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plot_silhouettes(X_norm, clusters)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    Next, compare your 7 clusters to the 7 pre-assigned classes by computing the Completeness and Homogeneity \n",
    "    values of the generated clusters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6132074717141491\n"
     ]
    }
   ],
   "source": [
    "# Code for homogeneity and completeness taken from lecture notebooks\n",
    "from sklearn.metrics import completeness_score, homogeneity_score\n",
    "\n",
    "print(completeness_score(y,clusters))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6115130439578844\n"
     ]
    }
   ],
   "source": [
    "print(homogeneity_score(y,clusters))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    c. Perform PCA on the normalized image data matrix. \n",
    "    \n",
    "    You may use the linear algebra package in Numpy or the Decomposition module in scikit-learn (the latter is much more efficient). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.08  0.    0.   -0.   -0.   -0.    0.   -0.    0.    0.    0.    0.01  0.01 -0.01  0.    0.\n",
      "   0.01 -0.01  0.  ]\n",
      " [ 0.    0.06  0.    0.    0.    0.   -0.    0.   -0.   -0.03 -0.03 -0.03 -0.03  0.02 -0.02  0.02\n",
      "  -0.03  0.    0.04]\n",
      " [ 0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.\n",
      "   0.    0.    0.  ]\n",
      " [-0.    0.    0.    0.02 -0.   -0.   -0.   -0.   -0.   -0.   -0.   -0.   -0.    0.   -0.    0.\n",
      "  -0.   -0.    0.  ]\n",
      " [-0.    0.    0.   -0.    0.01  0.    0.    0.    0.   -0.   -0.   -0.   -0.   -0.    0.   -0.\n",
      "  -0.    0.   -0.  ]\n",
      " [-0.    0.    0.   -0.    0.    0.01  0.    0.    0.   -0.   -0.    0.   -0.   -0.    0.   -0.\n",
      "   0.   -0.   -0.  ]\n",
      " [ 0.   -0.    0.   -0.    0.    0.    0.    0.    0.   -0.   -0.    0.    0.   -0.    0.    0.\n",
      "   0.    0.   -0.  ]\n",
      " [-0.    0.    0.   -0.    0.    0.    0.    0.01  0.    0.    0.    0.    0.   -0.    0.   -0.\n",
      "   0.   -0.   -0.  ]\n",
      " [ 0.   -0.    0.   -0.    0.    0.    0.    0.    0.    0.    0.    0.    0.   -0.    0.   -0.\n",
      "   0.   -0.   -0.  ]\n",
      " [ 0.   -0.03  0.   -0.   -0.   -0.   -0.    0.    0.    0.07  0.07  0.08  0.07 -0.04  0.04 -0.03\n",
      "   0.08 -0.04 -0.02]\n",
      " [ 0.   -0.03  0.   -0.   -0.   -0.   -0.    0.    0.    0.07  0.07  0.07  0.06 -0.04  0.04 -0.03\n",
      "   0.07 -0.04 -0.02]\n",
      " [ 0.01 -0.03  0.   -0.   -0.    0.    0.    0.    0.    0.08  0.07  0.08  0.07 -0.05  0.05 -0.03\n",
      "   0.08 -0.04 -0.03]\n",
      " [ 0.01 -0.03  0.   -0.   -0.   -0.    0.    0.    0.    0.07  0.06  0.07  0.06 -0.04  0.04 -0.02\n",
      "   0.07 -0.04 -0.02]\n",
      " [-0.01  0.02  0.    0.   -0.   -0.   -0.   -0.   -0.   -0.04 -0.04 -0.05 -0.04  0.04 -0.03  0.02\n",
      "  -0.05  0.02  0.01]\n",
      " [ 0.   -0.02  0.   -0.    0.    0.    0.    0.    0.    0.04  0.04  0.05  0.04 -0.03  0.04 -0.03\n",
      "   0.05 -0.02 -0.03]\n",
      " [ 0.    0.02  0.    0.   -0.   -0.    0.   -0.   -0.   -0.03 -0.03 -0.03 -0.02  0.02 -0.03  0.04\n",
      "  -0.03  0.01  0.04]\n",
      " [ 0.01 -0.03  0.   -0.   -0.    0.    0.    0.    0.    0.08  0.07  0.08  0.07 -0.05  0.05 -0.03\n",
      "   0.08 -0.04 -0.03]\n",
      " [-0.01  0.    0.   -0.    0.   -0.    0.   -0.   -0.   -0.04 -0.04 -0.04 -0.04  0.02 -0.02  0.01\n",
      "  -0.04  0.05 -0.  ]\n",
      " [ 0.    0.04  0.    0.   -0.   -0.   -0.   -0.   -0.   -0.02 -0.02 -0.03 -0.02  0.01 -0.03  0.04\n",
      "  -0.03 -0.    0.07]]\n"
     ]
    }
   ],
   "source": [
    "# PCA code taken from lecture notebooks\n",
    "from sklearn import decomposition\n",
    "\n",
    "meanVals = np.mean(X_norm, axis=0)\n",
    "meanRemoved = X_norm - meanVals #remove mean\n",
    "covMat = np.cov(meanRemoved, rowvar=0)\n",
    "\n",
    "np.set_printoptions(precision=2,suppress=True,linewidth=100)\n",
    "print(covMat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Eigenvalues:  [0.48 0.1  0.08 0.04 0.03 0.02 0.01 0.01 0.01 0.01 0.   0.   0.   0.   0.   0.   0.   0.   0.  ]\n",
      "Eigenvectors: [[ 0.03 -0.35  0.93  0.04 -0.01  0.03  0.01  0.03 -0.   -0.02 -0.01  0.01 -0.    0.    0.   -0.\n",
      "  -0.   -0.    0.  ]\n",
      " [-0.19 -0.38 -0.12 -0.66 -0.47  0.14 -0.24  0.22  0.06  0.07  0.06 -0.04 -0.01 -0.01  0.    0.\n",
      "  -0.   -0.    0.  ]\n",
      " [ 0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.\n",
      "   0.    0.    1.  ]\n",
      " [-0.01 -0.03 -0.04 -0.03  0.06  0.6   0.72  0.32  0.06  0.08  0.01 -0.01 -0.   -0.    0.    0.\n",
      "   0.    0.    0.  ]\n",
      " [-0.    0.02  0.01 -0.1  -0.09 -0.44  0.28  0.32 -0.77  0.05  0.07 -0.    0.01  0.   -0.   -0.\n",
      "  -0.    0.    0.  ]\n",
      " [ 0.    0.02  0.01 -0.12 -0.03 -0.42  0.3   0.11  0.44 -0.34  0.55  0.28 -0.11 -0.01 -0.   -0.\n",
      "  -0.   -0.    0.  ]\n",
      " [ 0.    0.01  0.01 -0.01 -0.01 -0.18  0.12  0.07  0.18 -0.14  0.05 -0.77  0.55  0.01 -0.    0.\n",
      "  -0.    0.    0.  ]\n",
      " [ 0.01  0.   -0.   -0.14 -0.03 -0.33  0.19  0.16  0.29 -0.02 -0.77  0.29  0.21 -0.01 -0.   -0.\n",
      "   0.   -0.    0.  ]\n",
      " [ 0.    0.    0.   -0.02 -0.01 -0.16  0.09  0.07  0.13 -0.05 -0.24 -0.49 -0.8   0.01  0.   -0.\n",
      "  -0.    0.    0.  ]\n",
      " [ 0.38 -0.11 -0.06  0.09 -0.05  0.02 -0.08  0.16  0.01 -0.05  0.    0.   -0.   -0.21  0.74 -0.02\n",
      "   0.11  0.36  0.  ]\n",
      " [ 0.36 -0.11 -0.07  0.09  0.01  0.04 -0.13  0.24  0.   -0.11 -0.    0.01 -0.   -0.21 -0.06  0.5\n",
      "  -0.62  0.1   0.  ]\n",
      " [ 0.41 -0.07 -0.04  0.03 -0.08  0.03 -0.04  0.07 -0.01 -0.06 -0.01 -0.   -0.   -0.22 -0.16 -0.81\n",
      "  -0.16 -0.08  0.  ]\n",
      " [ 0.36 -0.16 -0.08  0.14 -0.06 -0.01 -0.08  0.18  0.04  0.02  0.02  0.   -0.   -0.21 -0.53  0.27\n",
      "   0.69 -0.39  0.  ]\n",
      " [-0.24  0.06  0.    0.01  0.39  0.13 -0.31  0.5  -0.05 -0.42 -0.04  0.03 -0.    0.08 -0.2  -0.11\n",
      "  -0.06  0.37  0.  ]\n",
      " [ 0.26  0.18  0.08 -0.27 -0.18  0.04  0.18 -0.4  -0.08 -0.08 -0.04 -0.02  0.   -0.08 -0.3   0.1\n",
      "  -0.2   0.62  0.  ]\n",
      " [-0.18 -0.34 -0.13  0.43 -0.11 -0.2   0.03  0.14  0.17  0.55  0.1   0.    0.    0.05 -0.13 -0.08\n",
      "  -0.24  0.43  0.  ]\n",
      " [ 0.41 -0.1  -0.06  0.04 -0.1   0.02 -0.03  0.06 -0.   -0.07 -0.01  0.02  0.    0.89 -0.   -0.\n",
      "  -0.   -0.    0.  ]\n",
      " [-0.2   0.31  0.08  0.42 -0.74  0.12 -0.03  0.13 -0.01 -0.3  -0.06  0.03  0.   -0.01 -0.    0.\n",
      "  -0.    0.    0.  ]\n",
      " [-0.17 -0.64 -0.24  0.2   0.01  0.02  0.18 -0.35 -0.18 -0.5  -0.12  0.02  0.   -0.04 -0.    0.\n",
      "  -0.   -0.    0.  ]]\n"
     ]
    }
   ],
   "source": [
    "import numpy.linalg as la\n",
    "eigVals,eigVects = la.eig(np.mat(covMat))\n",
    "print(\"Eigenvalues: \",eigVals)\n",
    "print(\"Eigenvectors:\",eigVects)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.48 0.1  0.08 0.04 0.03 0.02 0.01 0.01 0.01 0.01 0.   0.   0.   0.   0.   0.   0.   0.   0.  ]\n",
      "[60.71 13.2  10.12  4.54  3.55  1.99  1.89  1.62  1.07  0.71  0.39  0.16  0.05  0.    0.    0.\n",
      "  0.    0.    0.  ]\n"
     ]
    }
   ],
   "source": [
    "eigValInd = np.argsort(eigVals)  #sort, sort goes smallest to largest\n",
    "eigValInd = eigValInd[::-1]   #reverse\n",
    "sortedEigVals = eigVals[eigValInd]\n",
    "print(sortedEigVals)\n",
    "total = sum(sortedEigVals)\n",
    "varPercentage = sortedEigVals/total*100\n",
    "print(varPercentage)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    Analyze the principal components to determine the number, r, of PCs needed to capture at least 95% of variance in the data.     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 PCs: 60.714233968533236\n",
      "2 PCs: 73.91121320168925\n",
      "3 PCs: 84.03498614256189\n",
      "4 PCs: 88.57852534332582\n",
      "5 PCs: 92.12588648109565\n",
      "6 PCs: 94.1139219796062\n",
      "7 PCs: 96.00589227704954\n"
     ]
    }
   ],
   "source": [
    "total = 0\n",
    "for i in range(10):\n",
    "    total += varPercentage[i]\n",
    "    print(\"{} PCs: {}\".format(i+1, total))\n",
    "    if total >= 95: break\n",
    "        \n",
    "topNfeat = 7"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The above shows us that we will need 7 principal compents to capture 95% of the variance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    Provide a plot of PC variances."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(111)\n",
    "ax.plot(range(1, 11), varPercentage[:10], marker='^')\n",
    "plt.xlabel('Principal Component Number')\n",
    "plt.ylabel('Percentage of Variance')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    Then use these r components as features to transform the data into a reduced dimension space."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-0.69 -0.53 -0.25 ... -0.08 -0.05 -0.05]\n",
      " [-0.67 -0.51 -0.34 ... -0.04 -0.06 -0.04]\n",
      " [-0.71 -0.77  0.16 ... -0.17 -0.04 -0.06]\n",
      " ...\n",
      " [-0.51  0.13  0.08 ... -0.03  0.03 -0.11]\n",
      " [-0.48  0.09  0.16 ...  0.    0.   -0.09]\n",
      " [-0.44  0.11  0.05 ...  0.02  0.21  0.15]]\n"
     ]
    }
   ],
   "source": [
    "topEigValInd = eigValInd[:topNfeat]  #cut off unwanted dimensions\n",
    "reducedEigVects = eigVects[:,topEigValInd]   #reorganize eig vects largest to smallest\n",
    "reducedDT = np.dot(meanRemoved, reducedEigVects)    #transform data into new dimensions\n",
    "print(reducedDT)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    d. Perform Kmeans again, but this time on the lower dimensional transformed data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "KMeans(algorithm='auto', copy_x=True, init='k-means++', max_iter=1000,\n",
       "       n_clusters=7, n_init=10, n_jobs=None, precompute_distances='auto',\n",
       "       random_state=None, tol=0.0001, verbose=0)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "kmeans = KMeans(n_clusters=7,max_iter=1000,verbose=0)\n",
    "kmeans.fit(reducedDT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
       "       2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 3, 1, 3, 3, 3, 3, 1, 1, 3, 3, 3, 3, 3, 3,\n",
       "       3, 1, 3, 3, 3, 3, 1, 3, 1, 3, 1, 3, 3, 3, 3, 3, 1, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 1, 3, 1, 3,\n",
       "       3, 3, 3, 3, 3, 3, 3, 3, 3, 1, 3, 3, 3, 3, 3, 1, 3, 3, 3, 3, 3, 3, 3, 1, 3, 3, 1, 3, 3, 3, 3,\n",
       "       1, 3, 1, 3, 3, 3, 1, 1, 1, 3, 1, 3, 3, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6,\n",
       "       6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6,\n",
       "       6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6,\n",
       "       6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 1, 3, 3, 1, 3, 1, 1, 3, 6, 3, 1, 3, 1, 3, 1,\n",
       "       1, 1, 4, 1, 1, 3, 3, 1, 1, 1, 1, 3, 1, 3, 3, 1, 3, 3, 3, 1, 1, 1, 3, 4, 3, 1, 1, 1, 1, 1, 3,\n",
       "       3, 3, 3, 3, 4, 1, 1, 1, 4, 1, 3, 1, 3, 3, 1, 1, 1, 3, 1, 4, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 3,\n",
       "       3, 3, 1, 3, 3, 1, 3, 4, 1, 1, 3, 1, 1, 1, 3, 1, 4, 3, 1, 1, 1, 1, 3, 1, 4, 1, 1, 1, 1, 1, 3,\n",
       "       1, 4, 1, 3, 3, 1, 3, 1, 1, 3, 3, 3, 1, 1, 3, 1, 4, 1, 1, 1, 1, 1, 1, 1, 1, 3, 1, 3, 1, 4, 3,\n",
       "       1, 3, 1, 1, 1, 3, 4, 3, 1, 4, 3, 3, 3, 3, 3, 1, 1, 1, 3, 1, 3, 1, 0, 4, 0, 4, 4, 0, 4, 6, 4,\n",
       "       0, 4, 4, 4, 0, 0, 0, 0, 4, 4, 0, 0, 4, 4, 0, 4, 4, 4, 4, 4, 0, 0, 4, 0, 4, 4, 0, 4, 0, 0, 0,\n",
       "       0, 4, 0, 4, 0, 1, 0, 0, 1, 0, 4, 4, 0, 0, 4, 1, 4, 0, 0, 0, 0, 0, 4, 0, 0, 0, 4, 0, 4, 0, 6,\n",
       "       4, 4, 0, 0, 4, 0, 4, 0, 4, 4, 0, 0, 0, 0, 0, 0, 0, 4, 4, 4, 4, 0, 0, 0, 0, 0, 4, 4, 0, 5, 5,\n",
       "       5, 5, 5, 5], dtype=int32)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clusters = kmeans.predict(reducedDT)\n",
    "clusters[:500]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-0.60</td>\n",
       "      <td>0.36</td>\n",
       "      <td>-0.11</td>\n",
       "      <td>0.13</td>\n",
       "      <td>-0.13</td>\n",
       "      <td>0.02</td>\n",
       "      <td>-0.04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.44</td>\n",
       "      <td>0.10</td>\n",
       "      <td>-0.16</td>\n",
       "      <td>-0.23</td>\n",
       "      <td>-0.05</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-0.62</td>\n",
       "      <td>-0.64</td>\n",
       "      <td>-0.20</td>\n",
       "      <td>0.09</td>\n",
       "      <td>-0.07</td>\n",
       "      <td>-0.01</td>\n",
       "      <td>0.04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.18</td>\n",
       "      <td>-0.04</td>\n",
       "      <td>0.27</td>\n",
       "      <td>-0.18</td>\n",
       "      <td>0.03</td>\n",
       "      <td>-0.02</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-0.21</td>\n",
       "      <td>0.25</td>\n",
       "      <td>-0.15</td>\n",
       "      <td>-0.06</td>\n",
       "      <td>0.13</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>1.41</td>\n",
       "      <td>-0.09</td>\n",
       "      <td>-0.04</td>\n",
       "      <td>0.17</td>\n",
       "      <td>-0.03</td>\n",
       "      <td>0.01</td>\n",
       "      <td>-0.02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>-0.51</td>\n",
       "      <td>0.06</td>\n",
       "      <td>0.34</td>\n",
       "      <td>0.07</td>\n",
       "      <td>0.08</td>\n",
       "      <td>-0.01</td>\n",
       "      <td>-0.03</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      0     1     2     3     4     5     6\n",
       "0 -0.60  0.36 -0.11  0.13 -0.13  0.02 -0.04\n",
       "1  0.44  0.10 -0.16 -0.23 -0.05  0.01  0.02\n",
       "2 -0.62 -0.64 -0.20  0.09 -0.07 -0.01  0.04\n",
       "3  0.18 -0.04  0.27 -0.18  0.03 -0.02  0.00\n",
       "4 -0.21  0.25 -0.15 -0.06  0.13  0.01  0.03\n",
       "5  1.41 -0.09 -0.04  0.17 -0.03  0.01 -0.02\n",
       "6 -0.51  0.06  0.34  0.07  0.08 -0.01 -0.03"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.options.display.float_format='{:,.2f}'.format\n",
    "centroids = pd.DataFrame(kmeans.cluster_centers_)\n",
    "centroids"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    Then compare Silhouette values as well as completeness and Homogeneity values of the new clusters. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.35982029813508676\n"
     ]
    }
   ],
   "source": [
    "silhouettes = metrics.silhouette_samples(reducedDT, clusters)\n",
    "print(silhouettes.mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plot_silhouettes(reducedDT, clusters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6107955063694607\n"
     ]
    }
   ],
   "source": [
    "print(completeness_score(y,clusters))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.609136404973329\n"
     ]
    }
   ],
   "source": [
    "print(homogeneity_score(y,clusters))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    Compare these results with those obtained on the full data in part b."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The results are eerily similar, which is pretty incredible considering we just ditched 12 dimensions. In fact, the mean of the silhouettes is actually slightly higher with PCA, probably due to not overfitting the extraneous dimensions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__2. Item-Based Joke Recommendation [Dataset: jokes.zip]__\n",
    "\n",
    "For this problem you will use a modified version of the item-based recommender algorithm from Ch. 14 of Machine Learning in Action and use it on joke ratings data based on Jester Online Joke Recommender System. \n",
    "\n",
    "The modified version of the code is provided in the module itemBasedRec.py. \n",
    "\n",
    "Most of the module will be used as is, but you will add some additional functionality.\n",
    "\n",
    "The data set contains two files. The file \"modified_jester_data.csv\" contains the ratings on 100 jokes by 1000 users (each row is a user profile). \n",
    "\n",
    "The ratings have been normalized to be between 1 and 21 (a 20-point scale), with 1 being the lowest rating. A zero indicated a missing rating. The file \"jokes.csv\" contains the joke ids mapped to the actual text of the jokes.\n",
    "\n",
    "Your tasks in this problem are the following (please also see comments for the function stubs in the provided module):"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    a. Load in the joke ratings data and the joke text data into appropriate data structures. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_PATH = \"/Users/claytoncohn/Dropbox/New/DePaul/DSC478/data/jokes/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>90</th>\n",
       "      <th>91</th>\n",
       "      <th>92</th>\n",
       "      <th>93</th>\n",
       "      <th>94</th>\n",
       "      <th>95</th>\n",
       "      <th>96</th>\n",
       "      <th>97</th>\n",
       "      <th>98</th>\n",
       "      <th>99</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>3.18</td>\n",
       "      <td>19.79</td>\n",
       "      <td>1.34</td>\n",
       "      <td>2.84</td>\n",
       "      <td>3.48</td>\n",
       "      <td>2.50</td>\n",
       "      <td>1.15</td>\n",
       "      <td>15.17</td>\n",
       "      <td>2.02</td>\n",
       "      <td>6.24</td>\n",
       "      <td>...</td>\n",
       "      <td>13.82</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>5.37</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>15.08</td>\n",
       "      <td>10.71</td>\n",
       "      <td>17.36</td>\n",
       "      <td>15.37</td>\n",
       "      <td>8.62</td>\n",
       "      <td>1.34</td>\n",
       "      <td>10.27</td>\n",
       "      <td>5.66</td>\n",
       "      <td>19.88</td>\n",
       "      <td>20.22</td>\n",
       "      <td>...</td>\n",
       "      <td>13.82</td>\n",
       "      <td>6.05</td>\n",
       "      <td>10.71</td>\n",
       "      <td>18.86</td>\n",
       "      <td>10.81</td>\n",
       "      <td>8.86</td>\n",
       "      <td>14.06</td>\n",
       "      <td>11.34</td>\n",
       "      <td>6.68</td>\n",
       "      <td>12.07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>20.03</td>\n",
       "      <td>20.27</td>\n",
       "      <td>20.03</td>\n",
       "      <td>20.27</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>...</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>20.08</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.00</td>\n",
       "      <td>19.35</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>12.80</td>\n",
       "      <td>19.16</td>\n",
       "      <td>8.18</td>\n",
       "      <td>17.21</td>\n",
       "      <td>0.00</td>\n",
       "      <td>12.84</td>\n",
       "      <td>...</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>11.53</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>19.50</td>\n",
       "      <td>15.61</td>\n",
       "      <td>6.83</td>\n",
       "      <td>5.61</td>\n",
       "      <td>12.36</td>\n",
       "      <td>12.60</td>\n",
       "      <td>18.04</td>\n",
       "      <td>15.61</td>\n",
       "      <td>10.56</td>\n",
       "      <td>16.73</td>\n",
       "      <td>...</td>\n",
       "      <td>16.19</td>\n",
       "      <td>16.58</td>\n",
       "      <td>15.27</td>\n",
       "      <td>16.19</td>\n",
       "      <td>16.73</td>\n",
       "      <td>12.55</td>\n",
       "      <td>14.11</td>\n",
       "      <td>17.55</td>\n",
       "      <td>12.80</td>\n",
       "      <td>12.60</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>4.83</td>\n",
       "      <td>7.46</td>\n",
       "      <td>11.44</td>\n",
       "      <td>2.50</td>\n",
       "      <td>3.91</td>\n",
       "      <td>6.68</td>\n",
       "      <td>2.31</td>\n",
       "      <td>10.13</td>\n",
       "      <td>4.35</td>\n",
       "      <td>9.20</td>\n",
       "      <td>...</td>\n",
       "      <td>7.46</td>\n",
       "      <td>4.11</td>\n",
       "      <td>10.32</td>\n",
       "      <td>8.04</td>\n",
       "      <td>8.82</td>\n",
       "      <td>7.65</td>\n",
       "      <td>11.05</td>\n",
       "      <td>1.92</td>\n",
       "      <td>5.95</td>\n",
       "      <td>7.55</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>19.59</td>\n",
       "      <td>1.15</td>\n",
       "      <td>18.72</td>\n",
       "      <td>19.79</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>...</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>13.33</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>17.84</td>\n",
       "      <td>14.16</td>\n",
       "      <td>20.17</td>\n",
       "      <td>4.79</td>\n",
       "      <td>2.84</td>\n",
       "      <td>9.30</td>\n",
       "      <td>20.27</td>\n",
       "      <td>12.41</td>\n",
       "      <td>5.81</td>\n",
       "      <td>6.58</td>\n",
       "      <td>...</td>\n",
       "      <td>18.23</td>\n",
       "      <td>9.88</td>\n",
       "      <td>10.90</td>\n",
       "      <td>5.32</td>\n",
       "      <td>7.84</td>\n",
       "      <td>7.65</td>\n",
       "      <td>13.14</td>\n",
       "      <td>10.95</td>\n",
       "      <td>12.31</td>\n",
       "      <td>11.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>7.21</td>\n",
       "      <td>7.46</td>\n",
       "      <td>1.58</td>\n",
       "      <td>4.11</td>\n",
       "      <td>2.26</td>\n",
       "      <td>10.71</td>\n",
       "      <td>5.71</td>\n",
       "      <td>2.07</td>\n",
       "      <td>3.14</td>\n",
       "      <td>9.40</td>\n",
       "      <td>...</td>\n",
       "      <td>15.37</td>\n",
       "      <td>10.71</td>\n",
       "      <td>15.17</td>\n",
       "      <td>10.71</td>\n",
       "      <td>10.71</td>\n",
       "      <td>10.71</td>\n",
       "      <td>10.71</td>\n",
       "      <td>10.71</td>\n",
       "      <td>7.60</td>\n",
       "      <td>6.05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>14.01</td>\n",
       "      <td>16.15</td>\n",
       "      <td>16.15</td>\n",
       "      <td>14.01</td>\n",
       "      <td>17.41</td>\n",
       "      <td>16.15</td>\n",
       "      <td>19.93</td>\n",
       "      <td>13.52</td>\n",
       "      <td>14.01</td>\n",
       "      <td>19.16</td>\n",
       "      <td>...</td>\n",
       "      <td>0.00</td>\n",
       "      <td>15.47</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>10 rows  100 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     0     1     2     3     4     5     6     7     8     9   ...    90  \\\n",
       "0  3.18 19.79  1.34  2.84  3.48  2.50  1.15 15.17  2.02  6.24  ... 13.82   \n",
       "1 15.08 10.71 17.36 15.37  8.62  1.34 10.27  5.66 19.88 20.22  ... 13.82   \n",
       "2  0.00  0.00  0.00  0.00 20.03 20.27 20.03 20.27  0.00  0.00  ...  0.00   \n",
       "3  0.00 19.35  0.00  0.00 12.80 19.16  8.18 17.21  0.00 12.84  ...  0.00   \n",
       "4 19.50 15.61  6.83  5.61 12.36 12.60 18.04 15.61 10.56 16.73  ... 16.19   \n",
       "5  4.83  7.46 11.44  2.50  3.91  6.68  2.31 10.13  4.35  9.20  ...  7.46   \n",
       "6  0.00  0.00  0.00  0.00 19.59  1.15 18.72 19.79  0.00  0.00  ...  0.00   \n",
       "7 17.84 14.16 20.17  4.79  2.84  9.30 20.27 12.41  5.81  6.58  ... 18.23   \n",
       "8  7.21  7.46  1.58  4.11  2.26 10.71  5.71  2.07  3.14  9.40  ... 15.37   \n",
       "9 14.01 16.15 16.15 14.01 17.41 16.15 19.93 13.52 14.01 19.16  ...  0.00   \n",
       "\n",
       "     91    92    93    94    95    96    97    98    99  \n",
       "0  0.00  0.00  0.00  0.00  0.00  5.37  0.00  0.00  0.00  \n",
       "1  6.05 10.71 18.86 10.81  8.86 14.06 11.34  6.68 12.07  \n",
       "2  0.00  0.00 20.08  0.00  0.00  0.00  0.00  0.00  0.00  \n",
       "3  0.00  0.00 11.53  0.00  0.00  0.00  0.00  0.00  0.00  \n",
       "4 16.58 15.27 16.19 16.73 12.55 14.11 17.55 12.80 12.60  \n",
       "5  4.11 10.32  8.04  8.82  7.65 11.05  1.92  5.95  7.55  \n",
       "6  0.00  0.00  0.00  0.00 13.33  0.00  0.00  0.00  0.00  \n",
       "7  9.88 10.90  5.32  7.84  7.65 13.14 10.95 12.31 11.00  \n",
       "8 10.71 15.17 10.71 10.71 10.71 10.71 10.71  7.60  6.05  \n",
       "9 15.47  0.00  0.00  0.00  0.00  0.00  0.00  0.00  0.00  \n",
       "\n",
       "[10 rows x 100 columns]"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = pd.read_csv(DATA_PATH + \"modified_jester_data.csv\", header=None)\n",
    "X.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>A man visits the doctor. The doctor says \"I ha...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>This couple had an excellent relationship goin...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>Q. What's 200 feet long and has 4 teeth? A. Th...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>Q. What's the difference between a man and a t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>Q. What's O. J. Simpson's Internet address? A....</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>5</td>\n",
       "      <td>Bill &amp; Hillary are on a trip back to Arkansas....</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>6</td>\n",
       "      <td>How many feminists does it take to screw in a ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>7</td>\n",
       "      <td>Q. Did you hear about the dyslexic devil worsh...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>8</td>\n",
       "      <td>A country guy goes into a city bar that has a ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>9</td>\n",
       "      <td>Two cannibals are eating a clown one turns to ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   0                                                  1\n",
       "0  0  A man visits the doctor. The doctor says \"I ha...\n",
       "1  1  This couple had an excellent relationship goin...\n",
       "2  2  Q. What's 200 feet long and has 4 teeth? A. Th...\n",
       "3  3  Q. What's the difference between a man and a t...\n",
       "4  4  Q. What's O. J. Simpson's Internet address? A....\n",
       "5  5  Bill & Hillary are on a trip back to Arkansas....\n",
       "6  6  How many feminists does it take to screw in a ...\n",
       "7  7  Q. Did you hear about the dyslexic devil worsh...\n",
       "8  8  A country guy goes into a city bar that has a ...\n",
       "9  9  Two cannibals are eating a clown one turns to ..."
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y = pd.read_csv(DATA_PATH + \"jokes.csv\", header=None)\n",
    "y.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sim functions taken from lecture notebook\n",
    "\n",
    "def euclidSim(inA,inB):\n",
    "    return 1.0/(1.0 + la.norm(inA - inB))\n",
    "\n",
    "def pearsonSim(inA,inB):\n",
    "    if len(inA) < 3 : return 1.0\n",
    "    return 0.5+0.5*np.corrcoef(inA, inB, rowvar = 0)[0][1]\n",
    "\n",
    "def cosineSim(inA,inB):\n",
    "    num = float(inA.T*inB)\n",
    "    denom = la.norm(inA)*la.norm(inB)\n",
    "    return 0.5+0.5*(num/denom)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# standEst taken from lecture notebook\n",
    "\n",
    "def standEst(dataMat, user, simMeas, item):\n",
    "    n = np.shape(dataMat)[1]\n",
    "    simTotal = 0.0; ratSimTotal = 0.0\n",
    "    for j in range(n):\n",
    "        userRating = dataMat[user,j]\n",
    "        if userRating == 0: \n",
    "            continue\n",
    "        overLap = np.nonzero(np.logical_and(dataMat[:,item]>0, dataMat[:,j]>0))[0]\n",
    "        if len(overLap) == 0: \n",
    "            similarity = 0\n",
    "        else: \n",
    "            similarity = simMeas(dataMat[overLap,item], dataMat[overLap,j])\n",
    "        #print('the %d and %d similarity is: %f' % (item, j, similarity))\n",
    "        simTotal += similarity\n",
    "        ratSimTotal += similarity * userRating\n",
    "    if simTotal == 0: return 0\n",
    "    else: return ratSimTotal/simTotal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# svdEst taken from lecture notebook\n",
    "\n",
    "def svdEst(dataMat, user, simMeas, item):\n",
    "    n = np.shape(dataMat)[1]\n",
    "    simTotal = 0.0; ratSimTotal = 0.0\n",
    "    data = np.mat(dataMat)\n",
    "    U,Sigma,VT = la.svd(data)\n",
    "    Sig4 = np.mat(np.eye(4)*Sigma[:4]) #arrange Sig4 into a diagonal matrix\n",
    "    xformedItems = data.T * U[:,:4] * Sig4.I  #create transformed items\n",
    "    for j in range(n):\n",
    "        userRating = data[user,j]\n",
    "        if userRating == 0 or j==item: continue\n",
    "        similarity = simMeas(xformedItems[item,:].T, xformedItems[j,:].T)\n",
    "        #print('the %d and %d similarity is: %f' % (item, j, similarity))\n",
    "        simTotal += similarity\n",
    "        ratSimTotal += similarity * userRating\n",
    "    if simTotal == 0: return 0\n",
    "    else: return ratSimTotal/simTotal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# recommend function taken from lecture notebooks\n",
    "\n",
    "def recommend(dataMat, user, N=3, simMeas=cosineSim, estMethod=standEst):\n",
    "    unratedItems = np.nonzero(dataMat[user,:].A==0)[1] #find unrated items \n",
    "    if len(unratedItems) == 0: return 'you rated everything'\n",
    "    itemScores = []\n",
    "    for item in unratedItems:\n",
    "        estimatedScore = estMethod(dataMat, user, simMeas, item)\n",
    "        itemScores.append((item, estimatedScore))\n",
    "    return sorted(itemScores, key=lambda jj: jj[1], reverse=True)[:N]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    Use the \"recommend\" function to provide top 5 joke recommendations for at least 2 users. \n",
    "    \n",
    "    Use both standard item-based collaborative filtering (based on the rating prediction function \"standEst\") and \n",
    "    the SVD-based version of the item-based CF (using \"svdEst\" as the prediction engine) to generate these \n",
    "    recommendations for the two users and note the differences. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "user1 = 42\n",
    "user2 = 45\n",
    "\n",
    "X_data = np.mat(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "rec_user1_standEst = recommend(X_data, user1, N=5, simMeas=cosineSim, estMethod=standEst)\n",
    "rec_user1_svdEst = recommend(X_data, user1, N=5, simMeas=cosineSim, estMethod=svdEst)\n",
    "\n",
    "rec_user2_standEst = recommend(X_data, user2, N=5, simMeas=cosineSim, estMethod=standEst)\n",
    "rec_user2_svdEst = recommend(X_data, user2, N=5, simMeas=cosineSim, estMethod=svdEst)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    You should show the text of the recommended jokes as well as the predicted ratings for each."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "User1: standEst\n",
      "\n",
      "Rank\tJoke Number\tPredicted Rating\n",
      "\n",
      "1\t     99\t\t14.625867625333052\n",
      "Q: What's the difference between greeting a Queen and greeting thePresident of the United  States?A: You only have to get on one knee to greet the queen.\n",
      "\n",
      "2\t     97\t\t14.621723838538415\n",
      "Age and Womanhood1. Between the ages of 13 and 18 ... She is like Africa virgin and unexplored. 2. Between the ages of 19 and 35 ... She is like Asia hot and exotic. 3. Between the ages of 36 and 45 ... She is like America fully explored breathtakingly beautiful and free with her resources.4. Between the ages of 46 and 56 ...She is like Europe exhausted but still has points of interest. 5. After 56 she is like Australia ...Everybody knows it's down there but who gives a damn?\n",
      "\n",
      "3\t     50\t\t14.620904524644963\n",
      "Did you hear that Clinton has announced there is a new national bird?  The spread eagle.\n",
      "\n",
      "4\t     51\t\t14.61968307812138\n",
      "Q: What do Monica Lewinsky and Bob Dole have in common?A: They were both upset when Bill finished first.\n",
      "\n",
      "5\t     33\t\t14.619446588281514\n",
      "Out in the backwoods of some midwestern state little Johnny arrives at school an hour late.Teacher: \"Why are you so late John? \"Johny : \"My big brother got shot in the ass.\"(the teacher corrects his speech)Teacher: \"Rectum.\"Johnny : \"Wrecked him!? Hell It damn near killed him!\" \n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"User1: standEst\\n\\nRank\\tJoke Number\\tPredicted Rating\\n\")\n",
    "for i in range(len(rec_user1_standEst)):\n",
    "    print(\"{}\\t     {}\\t\\t{}\\n{}\\n\".format(i+1,rec_user1_standEst[i][0],rec_user1_standEst[i][1],y.iloc[rec_user1_standEst[i][0]][1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "User1: svdEst\n",
      "\n",
      "Rank\tJoke Number\tPredicted Rating\n",
      "\n",
      "1\t     38\t\t15.073065233770512\n",
      "What is the difference between men and women:A woman wants one man to satisfy her every need.A man wants every woman to satisfy his one need.\n",
      "\n",
      "2\t     11\t\t15.050019172280996\n",
      "A guy stood over his tee shot for what seemed an eternity looking up looking down measuring the distance figuring the wind direction and speed. Driving his partner nuts.Finally his exasperated partner says \"What the hell is taking so long? Hit the goddamn ball!\"The guy answers \"My wife is up there watching me from the clubhouse. I want to make this a perfect shot.\"\"Well hell man you don't stand a snowball's chance in hell of hitting her from here!\" \n",
      "\n",
      "3\t     51\t\t15.032975461919149\n",
      "Q: What do Monica Lewinsky and Bob Dole have in common?A: They were both upset when Bill finished first.\n",
      "\n",
      "4\t     5\t\t15.008523633117134\n",
      "Bill & Hillary are on a trip back to Arkansas. They're almost out of gas so Bill pulls into a service station on the outskirts of town. The attendant runs out of the station to serve them when Hillary realizes it's an old boyfriend from high school. She and the attendant chat as he gases up their car and cleans the windows. Then they all say good-bye. As Bill pulls the car onto the road he turns to Hillary and says 'Now aren't you glad you married me and not him ? You could've been the wife of a grease monkey !' To which Hillary replied 'No Bill. If I would have married him you'd be pumping gas and he would be the President !' \n",
      "\n",
      "5\t     50\t\t14.966714081152459\n",
      "Did you hear that Clinton has announced there is a new national bird?  The spread eagle.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"User1: svdEst\\n\\nRank\\tJoke Number\\tPredicted Rating\\n\")\n",
    "for i in range(len(rec_user1_svdEst)):\n",
    "    print(\"{}\\t     {}\\t\\t{}\\n{}\\n\".format(i+1,rec_user1_svdEst[i][0],rec_user1_svdEst[i][1],y.iloc[rec_user1_svdEst[i][0]][1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "User2: standEst\n",
      "\n",
      "Rank\tJoke Number\tPredicted Rating\n",
      "\n",
      "1\t     71\t\t11.222517642758593\n",
      "On the first day of college the Dean addressed the students pointing out some of the rules:\"The female dormitory will be out-of-bounds for all male students and the male dormitory to the female students. Anybody caught breaking this rule will be fined $20 the first time.\" He continued \"Anybody caught breaking this rule the second time will be fined $60. Being caught a third time will cost you a fine of $180. Are there any questions ?\"At this point a male student in the crowd inquired:\"How much for a season pass ?\"\n",
      "\n",
      "2\t     75\t\t11.21905070910725\n",
      "There once was a man and a woman that both  got in  a terrible car wreck. Both of their vehicles  were completely destroyed buy fortunately no one  was   hurt.  In thankfulness the woman said to the man 'We are both okay so we should celebrate. I have   a  bottle of wine in my car let's open it.' So the woman got the bottle out of the car and  handed it to the man. The man took a really big drink and handed the woman the bottle. The  woman  closed the bottle and put it down. The man  asked  'Aren't you going to take a drink?' The woman cleverly replied 'No I think I'll  just  wait for the cops to get here.'\n",
      "\n",
      "3\t     77\t\t11.211098556588654\n",
      "Q: What's the difference between the government  and  the Mafia? A: One of them is organized.\n",
      "\n",
      "4\t     73\t\t11.211080048887407\n",
      "Q: How many stalkers does it take to change a light bulb?A: Two. One to replace the bulb and the other to watch it day and night.\n",
      "\n",
      "5\t     74\t\t11.210129793426571\n",
      "Q: Do you know the difference between an intelligent male and theSasquatch? A: There have been actual reported sightings of the Sasquatch.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"User2: standEst\\n\\nRank\\tJoke Number\\tPredicted Rating\\n\")\n",
    "for i in range(len(rec_user2_standEst)):\n",
    "    print(\"{}\\t     {}\\t\\t{}\\n{}\\n\".format(i+1,rec_user2_standEst[i][0],rec_user2_standEst[i][1],y.iloc[rec_user2_standEst[i][0]][1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "User2: svdEst\n",
      "\n",
      "Rank\tJoke Number\tPredicted Rating\n",
      "\n",
      "1\t     70\t\t11.07362956769775\n",
      "At a recent Sacramento PC Users Group meeting a company was demonstrating its latest speech-recognition software.   A representative from the company was just about ready to start the demonstration and asked everyone in the room to quiet down.Just then someone in the back of the room yelled\"Format C: Return.\"Someone else chimed in:\"Yes Return\"Unfortunately the software worked.\n",
      "\n",
      "2\t     73\t\t11.071105120448296\n",
      "Q: How many stalkers does it take to change a light bulb?A: Two. One to replace the bulb and the other to watch it day and night.\n",
      "\n",
      "3\t     75\t\t11.060817258401034\n",
      "There once was a man and a woman that both  got in  a terrible car wreck. Both of their vehicles  were completely destroyed buy fortunately no one  was   hurt.  In thankfulness the woman said to the man 'We are both okay so we should celebrate. I have   a  bottle of wine in my car let's open it.' So the woman got the bottle out of the car and  handed it to the man. The man took a really big drink and handed the woman the bottle. The  woman  closed the bottle and put it down. The man  asked  'Aren't you going to take a drink?' The woman cleverly replied 'No I think I'll  just  wait for the cops to get here.'\n",
      "\n",
      "4\t     77\t\t11.05514757299987\n",
      "Q: What's the difference between the government  and  the Mafia? A: One of them is organized.\n",
      "\n",
      "5\t     71\t\t11.052826439936062\n",
      "On the first day of college the Dean addressed the students pointing out some of the rules:\"The female dormitory will be out-of-bounds for all male students and the male dormitory to the female students. Anybody caught breaking this rule will be fined $20 the first time.\" He continued \"Anybody caught breaking this rule the second time will be fined $60. Being caught a third time will cost you a fine of $180. Are there any questions ?\"At this point a male student in the crowd inquired:\"How much for a season pass ?\"\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"User2: svdEst\\n\\nRank\\tJoke Number\\tPredicted Rating\\n\")\n",
    "for i in range(len(rec_user2_svdEst)):\n",
    "    print(\"{}\\t     {}\\t\\t{}\\n{}\\n\".format(i+1,rec_user2_svdEst[i][0],rec_user2_svdEst[i][1],y.iloc[rec_user2_svdEst[i][0]][1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    b. Complete the definition for the function \"test\". \n",
    "    \n",
    "    This function iterates over all users and for each performs evaluation (by calling the provided \n",
    "    \"cross_validate_user\" function), and returns the error information necessary to compute \n",
    "    Mean Absolute Error (MAE)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.insert(0, DATA_PATH)\n",
    "\n",
    "import itemBasedRec as ibr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1000, 100)"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test template taken from itemBasedRec.py\n",
    "\n",
    "def test(dataMat, test_ratio, estMethod, simMeas=pearsonSim):\n",
    "    # Write this function to iterate over all users and for each perform evaluation by calling\n",
    "    # the above cross_validate_user function on each user. MAE will be the ratio of total error \n",
    "    # across all test cases to the total number of test cases, across all users\n",
    "    \n",
    "    # Must keep running count of MAE and number of test cases\n",
    "    total_error = 0\n",
    "    total_count = 0\n",
    "\n",
    "    # Iterate through \n",
    "    for i in range(len(dataMat)):\n",
    "        error_u, count_u = ibr.cross_validate_user(dataMat, i, test_ratio, eval(estMethod), simMeas)\n",
    "        total_error += error_u\n",
    "        total_count += count_u\n",
    "    \n",
    "    MAE = total_error/total_count\n",
    "    \n",
    "    return MAE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    Use this function to perform evaluation (with 20% test-ratio for each user) comparing MAE results using the \n",
    "    rating prediction function \"standEst\" with results using the \"svdEst\" prediction function. \n",
    "    \n",
    "    [Note: See comments provided in the module for hints on accomplishing these tasks.]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MAE standEst:\t 3.7340359866204675\n",
      "MAE svdEst:\t 3.60833937781394\n"
     ]
    }
   ],
   "source": [
    "mae_standEst = test(X, 0.2, \"standEst\")\n",
    "mae_svdEst = test(X, 0.2, \"svdEst\")\n",
    "print(\"MAE standEst:\\t\",mae_standEst)\n",
    "print(\"MAE svdEst:\\t\",mae_svdEst)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    c. Write a new function \"print_most_similar_jokes\" which takes the joke ratings data, a query joke id, \n",
    "    a parameter k for the number similar jokes, and a similarity metric function, and prints the text of the query joke as well as the texts of the top k most similar jokes based on user ratings. \n",
    "    \n",
    "    [Note: For hints on how to accomplish this task, please see comments at the end of the provided module as well as comments for the provided stub function.]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "# template for print_most_similar_jokes taken from provided module\n",
    "\n",
    "def print_most_similar_jokes(dataMat, jokes, queryJoke, k, metric=pearsonSim):\n",
    "    # Write this function to find the k most similar jokes (based on user ratings) to a queryJoke\n",
    "    # The queryJoke is a joke id as given in the 'jokes.csv' file (an corresponding to the a column in dataMat)\n",
    "    # You must compare ratings for the queryJoke (the column in dataMat corresponding to the joke), to all\n",
    "    # other joke rating vectors and return the top k. Note that this is the same as performing KNN on the \n",
    "    # columns of dataMat. The function must retrieve the text of the joke from 'jokes.csv' file and print both\n",
    "    # the queryJoke text as well as the text of the returned top-k jokes.\n",
    "    \n",
    "    # Transpose matrix so rows are jokes\n",
    "    jokeMat = dataMat.T\n",
    "    \n",
    "    # Store tuples of (distance, joke ID) and then sort\n",
    "    sims = []\n",
    "    \n",
    "    # Iterate through the jokes\n",
    "    for i in range(len(jokeMat)):\n",
    "        \n",
    "        # Ignore queryJoke\n",
    "        if (i == queryJoke): continue;\n",
    "        \n",
    "        # Get the similarity between the query joke and joke i\n",
    "        sim = metric(jokeMat[queryJoke], jokeMat[i])\n",
    "        \n",
    "        # Add the similarity and joke id to the array\n",
    "        sims.append((sim,i))\n",
    "    \n",
    "    # Sort the list descending by similarity\n",
    "    sims.sort(reverse=True)\n",
    "        \n",
    "    print(\"Top {} most similar jokes to joke {}:\\n\\n{}\".format(k,queryJoke,jokes.iloc[queryJoke][1]))\n",
    "    for i in range(k):\n",
    "        joke = sims[i]\n",
    "        print(\"\\nJoke {}, similarity {:0.4f}:\".format(i+1,joke[0]))\n",
    "        print(jokes.iloc[joke[1]][1])\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 5 most similar jokes to joke 0:\n",
      "\n",
      "A man visits the doctor. The doctor says \"I have bad news for you.You have cancer and Alzheimer's disease\". The man replies \"Well thank God I don't have cancer!\"\n",
      "\n",
      "Joke 1, similarity 0.8155:\n",
      "Q. What's the difference between a man and a toilet? A. A toilet doesn't follow you around after you use it.\n",
      "\n",
      "Joke 2, similarity 0.8147:\n",
      "Q: What do Monica Lewinsky and Bob Dole have in common?A: They were both upset when Bill finished first.\n",
      "\n",
      "Joke 3, similarity 0.8029:\n",
      "Q: How do you keep a computer programmer in the shower all day long?A: Give them a shampoo with a label that says\"rinse lather repeat\".\n",
      "\n",
      "Joke 4, similarity 0.8012:\n",
      "A woman has twins and gives them up for adoption.  One of them goes to a family in Egypt and is named \"Amal.\"  The other goes to a  family in Spain; they name him \"Juan.\"  Years later Juan sends a picture of himself to his mom.  Upon receiving the picture she tells her husband that she wishes she also had a picture of Amal.  Her husband responds \"But they are twins-if you've seen Juan you've seen   Amal.\n",
      "\n",
      "Joke 5, similarity 0.7989:\n",
      "What's the difference between a MacIntosh and anEtch-A-Sketch? You don't have to shake the Mac to clear the screen. \n"
     ]
    }
   ],
   "source": [
    "print_most_similar_jokes(X,y,0,5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    d. [Extra Credit]: Develop your own item-based collaborative filtering recommender that uses a \n",
    "    model-based approach (separating the training and the prediction tasks). \n",
    "    \n",
    "    In the training component, item-item similarities for all pairs of items are computed and stored in an \n",
    "    appropriate data structure. \n",
    "    \n",
    "    Your training function should be able to use different similarity functions (passed as a parameter) \n",
    "    including Cosine Similarity or Pearson Correlation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>990</th>\n",
       "      <th>991</th>\n",
       "      <th>992</th>\n",
       "      <th>993</th>\n",
       "      <th>994</th>\n",
       "      <th>995</th>\n",
       "      <th>996</th>\n",
       "      <th>997</th>\n",
       "      <th>998</th>\n",
       "      <th>999</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>3.18</td>\n",
       "      <td>15.08</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>19.50</td>\n",
       "      <td>4.83</td>\n",
       "      <td>0.00</td>\n",
       "      <td>17.84</td>\n",
       "      <td>7.21</td>\n",
       "      <td>14.01</td>\n",
       "      <td>...</td>\n",
       "      <td>0.00</td>\n",
       "      <td>13.38</td>\n",
       "      <td>13.09</td>\n",
       "      <td>0.00</td>\n",
       "      <td>14.54</td>\n",
       "      <td>12.94</td>\n",
       "      <td>15.27</td>\n",
       "      <td>16.58</td>\n",
       "      <td>3.67</td>\n",
       "      <td>9.88</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>19.79</td>\n",
       "      <td>10.71</td>\n",
       "      <td>0.00</td>\n",
       "      <td>19.35</td>\n",
       "      <td>15.61</td>\n",
       "      <td>7.46</td>\n",
       "      <td>0.00</td>\n",
       "      <td>14.16</td>\n",
       "      <td>7.46</td>\n",
       "      <td>16.15</td>\n",
       "      <td>...</td>\n",
       "      <td>0.00</td>\n",
       "      <td>20.27</td>\n",
       "      <td>7.17</td>\n",
       "      <td>0.00</td>\n",
       "      <td>16.83</td>\n",
       "      <td>5.47</td>\n",
       "      <td>11.39</td>\n",
       "      <td>16.63</td>\n",
       "      <td>4.45</td>\n",
       "      <td>11.73</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1.34</td>\n",
       "      <td>17.36</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>6.83</td>\n",
       "      <td>11.44</td>\n",
       "      <td>0.00</td>\n",
       "      <td>20.17</td>\n",
       "      <td>1.58</td>\n",
       "      <td>16.15</td>\n",
       "      <td>...</td>\n",
       "      <td>0.00</td>\n",
       "      <td>13.57</td>\n",
       "      <td>9.16</td>\n",
       "      <td>0.00</td>\n",
       "      <td>8.38</td>\n",
       "      <td>16.19</td>\n",
       "      <td>16.39</td>\n",
       "      <td>15.85</td>\n",
       "      <td>3.67</td>\n",
       "      <td>9.16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2.84</td>\n",
       "      <td>15.37</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>5.61</td>\n",
       "      <td>2.50</td>\n",
       "      <td>0.00</td>\n",
       "      <td>4.79</td>\n",
       "      <td>4.11</td>\n",
       "      <td>14.01</td>\n",
       "      <td>...</td>\n",
       "      <td>0.00</td>\n",
       "      <td>9.98</td>\n",
       "      <td>8.62</td>\n",
       "      <td>0.00</td>\n",
       "      <td>13.57</td>\n",
       "      <td>5.51</td>\n",
       "      <td>5.37</td>\n",
       "      <td>7.89</td>\n",
       "      <td>3.67</td>\n",
       "      <td>9.50</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>3.48</td>\n",
       "      <td>8.62</td>\n",
       "      <td>20.03</td>\n",
       "      <td>12.80</td>\n",
       "      <td>12.36</td>\n",
       "      <td>3.91</td>\n",
       "      <td>19.59</td>\n",
       "      <td>2.84</td>\n",
       "      <td>2.26</td>\n",
       "      <td>17.41</td>\n",
       "      <td>...</td>\n",
       "      <td>12.07</td>\n",
       "      <td>2.02</td>\n",
       "      <td>7.65</td>\n",
       "      <td>13.96</td>\n",
       "      <td>17.84</td>\n",
       "      <td>6.92</td>\n",
       "      <td>7.41</td>\n",
       "      <td>14.40</td>\n",
       "      <td>9.40</td>\n",
       "      <td>13.52</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows  1000 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    0     1     2     3     4     5     6     7    8     9    ...   990   991  \\\n",
       "0  3.18 15.08  0.00  0.00 19.50  4.83  0.00 17.84 7.21 14.01  ...  0.00 13.38   \n",
       "1 19.79 10.71  0.00 19.35 15.61  7.46  0.00 14.16 7.46 16.15  ...  0.00 20.27   \n",
       "2  1.34 17.36  0.00  0.00  6.83 11.44  0.00 20.17 1.58 16.15  ...  0.00 13.57   \n",
       "3  2.84 15.37  0.00  0.00  5.61  2.50  0.00  4.79 4.11 14.01  ...  0.00  9.98   \n",
       "4  3.48  8.62 20.03 12.80 12.36  3.91 19.59  2.84 2.26 17.41  ... 12.07  2.02   \n",
       "\n",
       "    992   993   994   995   996   997  998   999  \n",
       "0 13.09  0.00 14.54 12.94 15.27 16.58 3.67  9.88  \n",
       "1  7.17  0.00 16.83  5.47 11.39 16.63 4.45 11.73  \n",
       "2  9.16  0.00  8.38 16.19 16.39 15.85 3.67  9.16  \n",
       "3  8.62  0.00 13.57  5.51  5.37  7.89 3.67  9.50  \n",
       "4  7.65 13.96 17.84  6.92  7.41 14.40 9.40 13.52  \n",
       "\n",
       "[5 rows x 1000 columns]"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Get jokes as rows\n",
    "X = pd.read_csv(DATA_PATH + \"modified_jester_data.csv\", header=None)\n",
    "X = X.T\n",
    "X.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>990</th>\n",
       "      <th>991</th>\n",
       "      <th>992</th>\n",
       "      <th>993</th>\n",
       "      <th>994</th>\n",
       "      <th>995</th>\n",
       "      <th>996</th>\n",
       "      <th>997</th>\n",
       "      <th>998</th>\n",
       "      <th>999</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>78</th>\n",
       "      <td>0.00</td>\n",
       "      <td>19.64</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1.15</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1.87</td>\n",
       "      <td>10.71</td>\n",
       "      <td>0.00</td>\n",
       "      <td>...</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>17.60</td>\n",
       "      <td>15.90</td>\n",
       "      <td>7.55</td>\n",
       "      <td>4.06</td>\n",
       "      <td>0.00</td>\n",
       "      <td>7.21</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97</th>\n",
       "      <td>0.00</td>\n",
       "      <td>11.34</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>17.55</td>\n",
       "      <td>1.92</td>\n",
       "      <td>0.00</td>\n",
       "      <td>10.95</td>\n",
       "      <td>10.71</td>\n",
       "      <td>0.00</td>\n",
       "      <td>...</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>14.64</td>\n",
       "      <td>5.47</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>3.77</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>3.57</td>\n",
       "      <td>17.99</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>15.61</td>\n",
       "      <td>4.06</td>\n",
       "      <td>0.00</td>\n",
       "      <td>6.83</td>\n",
       "      <td>10.71</td>\n",
       "      <td>16.15</td>\n",
       "      <td>...</td>\n",
       "      <td>0.00</td>\n",
       "      <td>9.59</td>\n",
       "      <td>6.34</td>\n",
       "      <td>12.07</td>\n",
       "      <td>16.58</td>\n",
       "      <td>5.13</td>\n",
       "      <td>9.79</td>\n",
       "      <td>10.03</td>\n",
       "      <td>3.62</td>\n",
       "      <td>13.67</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>84</th>\n",
       "      <td>0.00</td>\n",
       "      <td>19.98</td>\n",
       "      <td>0.00</td>\n",
       "      <td>11.63</td>\n",
       "      <td>14.93</td>\n",
       "      <td>10.71</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1.10</td>\n",
       "      <td>7.60</td>\n",
       "      <td>15.56</td>\n",
       "      <td>...</td>\n",
       "      <td>12.21</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>15.95</td>\n",
       "      <td>7.89</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>4.16</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>2.60</td>\n",
       "      <td>10.13</td>\n",
       "      <td>20.08</td>\n",
       "      <td>18.18</td>\n",
       "      <td>15.51</td>\n",
       "      <td>5.61</td>\n",
       "      <td>0.00</td>\n",
       "      <td>4.93</td>\n",
       "      <td>15.42</td>\n",
       "      <td>14.01</td>\n",
       "      <td>...</td>\n",
       "      <td>16.05</td>\n",
       "      <td>14.45</td>\n",
       "      <td>10.81</td>\n",
       "      <td>14.83</td>\n",
       "      <td>14.40</td>\n",
       "      <td>7.80</td>\n",
       "      <td>15.76</td>\n",
       "      <td>11.24</td>\n",
       "      <td>3.38</td>\n",
       "      <td>14.25</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows  1000 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    0     1     2     3     4     5    6     7     8     9    ...   990   991  \\\n",
       "78 0.00 19.64  0.00  0.00  0.00  1.15 0.00  1.87 10.71  0.00  ...  0.00  0.00   \n",
       "97 0.00 11.34  0.00  0.00 17.55  1.92 0.00 10.95 10.71  0.00  ...  0.00  0.00   \n",
       "29 3.57 17.99  0.00  0.00 15.61  4.06 0.00  6.83 10.71 16.15  ...  0.00  9.59   \n",
       "84 0.00 19.98  0.00 11.63 14.93 10.71 0.00  1.10  7.60 15.56  ... 12.21  0.00   \n",
       "38 2.60 10.13 20.08 18.18 15.51  5.61 0.00  4.93 15.42 14.01  ... 16.05 14.45   \n",
       "\n",
       "     992   993   994  995   996   997  998   999  \n",
       "78  0.00 17.60 15.90 7.55  4.06  0.00 7.21  0.00  \n",
       "97  0.00  0.00 14.64 5.47  0.00  0.00 3.77  0.00  \n",
       "29  6.34 12.07 16.58 5.13  9.79 10.03 3.62 13.67  \n",
       "84  0.00  0.00 15.95 7.89  0.00  0.00 4.16  0.00  \n",
       "38 10.81 14.83 14.40 7.80 15.76 11.24 3.38 14.25  \n",
       "\n",
       "[5 rows x 1000 columns]"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Split data\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "SEED = 33\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=SEED)\n",
    "X_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getItemSims(data,metric=pearsonSim):\n",
    "    sims = []\n",
    "    for i in range(len(data)):\n",
    "        for j in range(i, len(data)):\n",
    "            \n",
    "            # Skip comparing similar jokes\n",
    "            if i == j: continue\n",
    "                \n",
    "            # Get the similarity between the two jokes\n",
    "            sim = metric(data[i], data[j])\n",
    "            item = (sim,i,j)\n",
    "            sims.append(item)\n",
    "    \n",
    "    # After all comparisons, sort\n",
    "    sims.sort(reverse=True)\n",
    "    return sims"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0.9366996388217678, 60, 67),\n",
       " (0.9281184407526155, 36, 67),\n",
       " (0.9211802501682792, 9, 60),\n",
       " (0.9192813981299488, 9, 67),\n",
       " (0.9107610183340304, 36, 60),\n",
       " (0.9090741242524762, 9, 36),\n",
       " (0.9033655670147729, 9, 22),\n",
       " (0.9015833108579466, 22, 51),\n",
       " (0.8948706176603536, 48, 60),\n",
       " (0.8928510919236995, 24, 60)]"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "item_sims = getItemSims(X_train)\n",
    "item_sims[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>90</th>\n",
       "      <th>91</th>\n",
       "      <th>92</th>\n",
       "      <th>93</th>\n",
       "      <th>94</th>\n",
       "      <th>95</th>\n",
       "      <th>96</th>\n",
       "      <th>97</th>\n",
       "      <th>98</th>\n",
       "      <th>99</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>3.18</td>\n",
       "      <td>19.79</td>\n",
       "      <td>1.34</td>\n",
       "      <td>2.84</td>\n",
       "      <td>3.48</td>\n",
       "      <td>2.50</td>\n",
       "      <td>1.15</td>\n",
       "      <td>15.17</td>\n",
       "      <td>2.02</td>\n",
       "      <td>6.24</td>\n",
       "      <td>...</td>\n",
       "      <td>13.82</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>5.37</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>15.08</td>\n",
       "      <td>10.71</td>\n",
       "      <td>17.36</td>\n",
       "      <td>15.37</td>\n",
       "      <td>8.62</td>\n",
       "      <td>1.34</td>\n",
       "      <td>10.27</td>\n",
       "      <td>5.66</td>\n",
       "      <td>19.88</td>\n",
       "      <td>20.22</td>\n",
       "      <td>...</td>\n",
       "      <td>13.82</td>\n",
       "      <td>6.05</td>\n",
       "      <td>10.71</td>\n",
       "      <td>18.86</td>\n",
       "      <td>10.81</td>\n",
       "      <td>8.86</td>\n",
       "      <td>14.06</td>\n",
       "      <td>11.34</td>\n",
       "      <td>6.68</td>\n",
       "      <td>12.07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>20.03</td>\n",
       "      <td>20.27</td>\n",
       "      <td>20.03</td>\n",
       "      <td>20.27</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>...</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>20.08</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.00</td>\n",
       "      <td>19.35</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>12.80</td>\n",
       "      <td>19.16</td>\n",
       "      <td>8.18</td>\n",
       "      <td>17.21</td>\n",
       "      <td>0.00</td>\n",
       "      <td>12.84</td>\n",
       "      <td>...</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>11.53</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>19.50</td>\n",
       "      <td>15.61</td>\n",
       "      <td>6.83</td>\n",
       "      <td>5.61</td>\n",
       "      <td>12.36</td>\n",
       "      <td>12.60</td>\n",
       "      <td>18.04</td>\n",
       "      <td>15.61</td>\n",
       "      <td>10.56</td>\n",
       "      <td>16.73</td>\n",
       "      <td>...</td>\n",
       "      <td>16.19</td>\n",
       "      <td>16.58</td>\n",
       "      <td>15.27</td>\n",
       "      <td>16.19</td>\n",
       "      <td>16.73</td>\n",
       "      <td>12.55</td>\n",
       "      <td>14.11</td>\n",
       "      <td>17.55</td>\n",
       "      <td>12.80</td>\n",
       "      <td>12.60</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>4.83</td>\n",
       "      <td>7.46</td>\n",
       "      <td>11.44</td>\n",
       "      <td>2.50</td>\n",
       "      <td>3.91</td>\n",
       "      <td>6.68</td>\n",
       "      <td>2.31</td>\n",
       "      <td>10.13</td>\n",
       "      <td>4.35</td>\n",
       "      <td>9.20</td>\n",
       "      <td>...</td>\n",
       "      <td>7.46</td>\n",
       "      <td>4.11</td>\n",
       "      <td>10.32</td>\n",
       "      <td>8.04</td>\n",
       "      <td>8.82</td>\n",
       "      <td>7.65</td>\n",
       "      <td>11.05</td>\n",
       "      <td>1.92</td>\n",
       "      <td>5.95</td>\n",
       "      <td>7.55</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>19.59</td>\n",
       "      <td>1.15</td>\n",
       "      <td>18.72</td>\n",
       "      <td>19.79</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>...</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>13.33</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>17.84</td>\n",
       "      <td>14.16</td>\n",
       "      <td>20.17</td>\n",
       "      <td>4.79</td>\n",
       "      <td>2.84</td>\n",
       "      <td>9.30</td>\n",
       "      <td>20.27</td>\n",
       "      <td>12.41</td>\n",
       "      <td>5.81</td>\n",
       "      <td>6.58</td>\n",
       "      <td>...</td>\n",
       "      <td>18.23</td>\n",
       "      <td>9.88</td>\n",
       "      <td>10.90</td>\n",
       "      <td>5.32</td>\n",
       "      <td>7.84</td>\n",
       "      <td>7.65</td>\n",
       "      <td>13.14</td>\n",
       "      <td>10.95</td>\n",
       "      <td>12.31</td>\n",
       "      <td>11.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>7.21</td>\n",
       "      <td>7.46</td>\n",
       "      <td>1.58</td>\n",
       "      <td>4.11</td>\n",
       "      <td>2.26</td>\n",
       "      <td>10.71</td>\n",
       "      <td>5.71</td>\n",
       "      <td>2.07</td>\n",
       "      <td>3.14</td>\n",
       "      <td>9.40</td>\n",
       "      <td>...</td>\n",
       "      <td>15.37</td>\n",
       "      <td>10.71</td>\n",
       "      <td>15.17</td>\n",
       "      <td>10.71</td>\n",
       "      <td>10.71</td>\n",
       "      <td>10.71</td>\n",
       "      <td>10.71</td>\n",
       "      <td>10.71</td>\n",
       "      <td>7.60</td>\n",
       "      <td>6.05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>14.01</td>\n",
       "      <td>16.15</td>\n",
       "      <td>16.15</td>\n",
       "      <td>14.01</td>\n",
       "      <td>17.41</td>\n",
       "      <td>16.15</td>\n",
       "      <td>19.93</td>\n",
       "      <td>13.52</td>\n",
       "      <td>14.01</td>\n",
       "      <td>19.16</td>\n",
       "      <td>...</td>\n",
       "      <td>0.00</td>\n",
       "      <td>15.47</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>10 rows  100 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     0     1     2     3     4     5     6     7     8     9   ...    90  \\\n",
       "0  3.18 19.79  1.34  2.84  3.48  2.50  1.15 15.17  2.02  6.24  ... 13.82   \n",
       "1 15.08 10.71 17.36 15.37  8.62  1.34 10.27  5.66 19.88 20.22  ... 13.82   \n",
       "2  0.00  0.00  0.00  0.00 20.03 20.27 20.03 20.27  0.00  0.00  ...  0.00   \n",
       "3  0.00 19.35  0.00  0.00 12.80 19.16  8.18 17.21  0.00 12.84  ...  0.00   \n",
       "4 19.50 15.61  6.83  5.61 12.36 12.60 18.04 15.61 10.56 16.73  ... 16.19   \n",
       "5  4.83  7.46 11.44  2.50  3.91  6.68  2.31 10.13  4.35  9.20  ...  7.46   \n",
       "6  0.00  0.00  0.00  0.00 19.59  1.15 18.72 19.79  0.00  0.00  ...  0.00   \n",
       "7 17.84 14.16 20.17  4.79  2.84  9.30 20.27 12.41  5.81  6.58  ... 18.23   \n",
       "8  7.21  7.46  1.58  4.11  2.26 10.71  5.71  2.07  3.14  9.40  ... 15.37   \n",
       "9 14.01 16.15 16.15 14.01 17.41 16.15 19.93 13.52 14.01 19.16  ...  0.00   \n",
       "\n",
       "     91    92    93    94    95    96    97    98    99  \n",
       "0  0.00  0.00  0.00  0.00  0.00  5.37  0.00  0.00  0.00  \n",
       "1  6.05 10.71 18.86 10.81  8.86 14.06 11.34  6.68 12.07  \n",
       "2  0.00  0.00 20.08  0.00  0.00  0.00  0.00  0.00  0.00  \n",
       "3  0.00  0.00 11.53  0.00  0.00  0.00  0.00  0.00  0.00  \n",
       "4 16.58 15.27 16.19 16.73 12.55 14.11 17.55 12.80 12.60  \n",
       "5  4.11 10.32  8.04  8.82  7.65 11.05  1.92  5.95  7.55  \n",
       "6  0.00  0.00  0.00  0.00 13.33  0.00  0.00  0.00  0.00  \n",
       "7  9.88 10.90  5.32  7.84  7.65 13.14 10.95 12.31 11.00  \n",
       "8 10.71 15.17 10.71 10.71 10.71 10.71 10.71  7.60  6.05  \n",
       "9 15.47  0.00  0.00  0.00  0.00  0.00  0.00  0.00  0.00  \n",
       "\n",
       "[10 rows x 100 columns]"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Get transposed matrix to evaluate users\n",
    "X_users = X.T\n",
    "X_users[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    The prediction (or estimation) function should take as parameters a target user, an item, a value of k, \n",
    "    and the similarities data structure and return the predicted rating on the target item for the target user. \n",
    "    \n",
    "    The predicted rating should be based on the weighted average of the target user's ratings on k most similar \n",
    "    items to the target item."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predictRating(user, joke, k, sims):\n",
    "        \n",
    "    # Isolate the item similarties pertinent to the joke in question (top k)\n",
    "    simsMat = [sims[i] for i in range(len(sims)) if sims[i][1] == joke or sims[i][2] == joke][:k]\n",
    "    \n",
    "    # Get indices of topK jokes\n",
    "    topK_ind = []\n",
    "    for i in range(len(simsMat)):\n",
    "        if simsMat[i][1] != joke:\n",
    "            topK_ind.append(simsMat[i][1])\n",
    "        else: \n",
    "            topK_ind.append(simsMat[i][2])\n",
    "    \n",
    "    # Get user's average rating of each topK joke\n",
    "    ratings = []\n",
    "    for n in topK_ind:\n",
    "        rating = X_users[user][n]\n",
    "        if rating != 0: ratings.append(rating)\n",
    "\n",
    "    # Return average of top k\n",
    "    return np.mean(ratings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "User 5 predicted 17.262 for joke 4.\n",
      "User 5 actual rating 12.6 for joke 4.\n"
     ]
    }
   ],
   "source": [
    "USER = 5\n",
    "JOKE = 4\n",
    "K = 5\n",
    "\n",
    "prediction = predictRating(USER, JOKE, K, item_sims)\n",
    "print(\"User {} predicted {} for joke {}.\".format(USER, prediction, JOKE))\n",
    "print(\"User {} actual rating {} for joke {}.\".format(USER, X_users[USER][JOKE], JOKE))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    You should test the prediction accuracy of your estimation function (using a cross-validation similar to \n",
    "    part b, above) and provide a plot of cross-validation accuracies across a range of values of k."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[16.68,\n",
       " 18.28,\n",
       " 18.896666666666665,\n",
       " 17.6375,\n",
       " 17.262,\n",
       " 17.076666666666668,\n",
       " 15.355714285714287,\n",
       " 15.831249999999999,\n",
       " 16.173333333333332,\n",
       " 16.438]"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Look at k in range(50)\n",
    "predictions = [predictRating(USER, JOKE, i, item_sims) for i in range(1,50) ]\n",
    "predictions[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[4.08,\n",
       " 5.6800000000000015,\n",
       " 6.296666666666665,\n",
       " 5.0375,\n",
       " 4.662000000000001,\n",
       " 4.476666666666668,\n",
       " 2.755714285714287,\n",
       " 3.2312499999999993,\n",
       " 3.5733333333333324,\n",
       " 3.837999999999999]"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Compare distance of actual joke and predicted joke\n",
    "dists = [abs(predictions[i] - X_users[USER][JOKE]) for i in range(len(predictions))]\n",
    "dists[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXwAAAD4CAYAAADvsV2wAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4xLjMsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+AADFEAAAgAElEQVR4nO3dd5wc1ZXo8d/pnpylmZFG0ignUERIICGRDMIEyyYJDAsOz7JlL9ik5fHAu14/wGnZXWyeA2tssDEmGAMi2IBBWICRAOUcUE4jzYzCaIImdp/3R3fDIE2o7umaTuf7+fSnu6qrq059NDp9+9ape0VVMcYYk/w8sQ7AGGNM77CEb4wxKcISvjHGpAhL+MYYkyIs4RtjTIpIi3UA7ZWUlOiwYcNiHYYxxiSMFStWHFLVUifbxlXCHzZsGMuXL491GMYYkzBEZLfTba1LxxhjUoQlfGOMSRGW8I0xJkVYwjfGmBRhCd8YY1KEJXxjjEkRlvCNMSZFpGTCb27z8fTSPfj8NjS0MSZ1pGTCX7ByP/e8sI4l2w/FOhRjjOk1KZnwF26qBGBHdUOMIzHGmN6Tcgm/scXHe9sCLfudhyzhG2NSR6dj6YjIOqCjTm4BVFUnuRaVixZvO0RTq58Mr4cdlvCNMSmkq8HT5vRaFL1o4aZK8jPTmDWqhA0HjsU6HGOM6TWdJnxVdTwCW6Lw+5WFm6o4b2wpI0rz+NvGgzS3+chM88Y6NGOMcV23ffgiMkNElolIvYi0iIhPRGp7I7hoW7OvhkP1zVw0rj8jSnJRhT2Hj8c6LGOM6RVOLtr+Arge2ApkA18Hfu5k5yJSJCLPichmEdkkImdFHmrPLdxUidcjnD+mHyNKcwHYbpU6xpgU4WgCFFXdJiJeVfUBvxORJQ73/xDwuqrOFZEMICfSQKNh4cYqzhzWl8KcdIZ5AgnfKnWMManCSQv/eDBZrxaRB0TkdiC3uw+JSAFwLvAogKq2qGpNj6Ltgb1HjrOlso7Z4/oDUJCVTkleJjsP1ccqJGOM6VVOEv6XAC/wbaABGAxc7eBzI4BqAr8IVonIb0XkpC8KEZkvIstFZHl1dXUYoYcndLPV7FP7fRJgSa618I0xKaPbhK+qu1W1UVVrVfVeVb1DVbc52HcacDrwsKpOIfBlcXcH+39EVaep6rTSUkfz8EZk4aZKRvfLY2jxJ985wy3hG2NSiJMqnZ0isuPEh4N97wP2qeqHweXnCHwB9Lpjja18uOPIx905IcNLczlU38KxxtZYhGWMMb3KyUXbae1eZwHXAH27+5CqHhSRvSIyVlW3ABcCGyMLs2fe+aiaNr8y+9QTEn5JoLW/61ADkwcXxSI0Y4zpNU66dA63e+xX1Z8BFzjc/3eAJ0VkLXAa8KMexBqxhRsrKcnL4LQTkvrIUqvUMcakjm5b+CLSvhvGQ6DFn+9k56q6mk//Quh1rT4/i7ZUcemEMrwe+dR7g/vm4BHYUW2VOsaY5OekS+e/271uA3YC17oTTvQt23mEuqa2k7pzADLTvJT3ybFB1IwxKcFJwp+nqp+6SCsiw12KJ+re3FRJZpqHs0eXdPi+VeoYY1KFkzr85xyuizuqysJNlcwaVUJORsffbaGEr2rTHRpjkltX4+GfAowHCkXkqnZvFRCo1ol7W6vq2XukkX8+b1Sn24wozeV4i4+qumb6FyTEaRljTES66tIZS2BM/CLg8+3W1wHfcDOoaHlzY+Du2gvb3V17ohEleUBgukNL+MaYZNbVePgvAS+JyFmq+n4vxhQ16/cfY0RJbpeJfHi70syzRhb3VmjGGNPrnPThf0tEPi5gF5E+IvKYizFFTUVNI4P6ZHe5zYCCLDLTPFaaaYxJek4S/qT2o1yq6lFginshRc/+mibKu0n4Ho9YpY4xJiU4SfgeEekTWhCRvjgcRz+Wmlp9HKpvZmBh1wkfrDTTGJManN54tUREQqWY1wA/dC+k6Dh4rAmAgUXOEv6bGytp9flJ9zr5DjTGmMTjZCydPwBzgUqgCrhKVZ9wO7CeqqhpBJwl/BGlebT5lX1HG90OyxhjYsbpFIcbRKSaYP29iAxR1T2uRtZD+4MJf5DDFj7AzkP1H782xphk42Q8/C+IyFYCY+i8A+wCXnM5rh6rqGlCBPoXZna77Yhgkt9hE5obY5KYkw7r+4EZwEeqOpzAuPaLXY0qCipqGinNyyQzzdvttn1yMyjKSbcLt8aYpOYk4beq6mEC1ToeVV1EYGz7uFZxrNFR/33I8JJca+EbY5Kakz78GhHJA94lMJlJFYFhkuPa/ppGTi0rcLz98JJclmw77GJExhgTW05a+JcDx4HbgdeB7Xx6bJ24o6pU1DQysMj52DgjSnI5WNtEQ3Pcf5cZY0xEum3hq2qon8MPPO5uONFx9HgrTa3+sLp0RpQGBlHbdbiB8QML3QrNGGNiJinvMgqnBj/kk9JM68c3xiSnpEz44dTghwwrDiZ8u3BrjElSSZnwI2nhZ2d4GViYZS18Y0zS6mrGq3VAp/P+qeokVyKKgoqaRrLSPfTJSQ/rc8NLc9luCd8Yk6S6umg7J/h8c/A5NH7ODQSqduJWRU0TA4uyEZGwPje8JJeXV1egqmF/1hhj4l1XM17tBhCRWao6q91bd4vIYuA+t4OL1P6axrD670NGlORR29TGkYYWivO6H5LBGGMSiZM+/FwROTu0ICIzgbgeYayiptHROPgnaj/doTHGJBsnd9rOAx4TkVBxeg3wNSc7F5FdBCY99wFtqjotkiDD0dzmo6quOawLtiEfD6J2qIFpw/pGOzRjjIkpJzderQAmi0gBIKp6LMxjfEZVD0UUXQQqjzUDhHWXbcigomzSvcLGitpoh2WMMTHXbcIXkUzgamAYkBa6mKmqcdmHH0kNfkia18PsU/vz/Ip93DZ7NEU5GdEOzxhjYsZJH/5LBMbTaQMa2j2cUOANEVkhIvM72kBE5ovIchFZXl1d7XC3nYukBr+9Wy4cTV1zG4+9t7PHsRhjTDxx0odfrqqXRLj/WapaISL9gDdFZLOqvtt+A1V9BHgEYNq0aZ3W/TsVauGXFYbfpQNw6oACLp1Qxu8W7+JrZw+3Vr4xJmk4aeEvEZGJkexcVSuCz1XAAuDMSPYTjoqaRkryMslK737ik85YK98Yk4ycJPyzgRUiskVE1orIOhFZ292HRCRXRPJDr4HPAut7Fm73AjX4kbXuQ9q38muOt0QpMmOMiS0nXTqXRrjv/sCC4EXeNOApVX09wn05VlHTyJj++T3ezy0Xjua19Qd57L2d3PHZsVGIzBhjYqvbFr6q7g7eddtI4CJs6NHd53ao6uTgY7yq/rDn4XZ7zI+HVegpa+UbY5JNtwlfRL4gIluBncA7wC7gNZfjikjN8VYaW31RSfhgffnGmOTipA//fmAG8JGqDgcuBBa7GlWEPqnB71kffoi18o0xycRJwm9V1cOAR0Q8qroIOM3luCLS0xr8jlgr3xiTLJwk/BoRyQPeBZ4UkYcI3IQVd9xI+NbKN8YkCycJ/3IC49/fDrwObAc+72ZQkao41kRmmofi3OjeLGWtfGNMMnAyeFpoGAU/8Li74fRMaBz8aE9eEmrl/8+7Ozh9aB/OH9svqvs3xpjekFRz2lbUNEa1O6e9H105kdH98pj/hxUs2lzlyjGMMcZNSZjwo1Ohc6I+uRk8+fXpjC3LZ/4Ty1m4sdKV4xhjjFuSJuG3tPkjnvjEqaKcDP44bzrjBhTwz0+u4I0NB107ljHGRFunCT80Zk5nj94M0onK2iZUo1uh05HCnHT+MG864wcWctOTK3l9/QFXj2eMMdHSVQt/DoFqnNeDjxuCj1eB59wPLTw9mfgkXIXZ6Twx70wmlRdy81OreHWdJX1jTPzrNOG3G0Nnlqreparrgo+7gYt7L0Rn3KjB70p+VqClP2VwEd95ehXbq+t75bjGGBMpJ334uSJydmhBRGYCue6FFJlQwh8Q4cQnkcjLTOP+Kybg8yubD9T12nGNMSYSToZHngc8JiKFBEbJPAZ8zdWoIrC/pomSvIweTXwSiYGFgV8UB4419upxjTEmXE5uvFoBTBaRAkBU9Zj7YYXPzRr8rhRkp5Gd7uXgsaZeP7YxxoTDyfDI/UXkUeBPqnpMRMaJyLxeiC0sFTWNH7e2e5OIMKAwiwO1lvCNMfHNSR/+74G/AQODyx8Bt7kVUCQCE5/EpoUPgQnTrYVvjIl3ThJ+iao+S2AsHVS1DfC5GlWYahvbaGjxuXaXbXcs4RtjEoGThN8gIsUEpzUUkRkELtzGjd6swe/IgMIsKmub8Pu7nfnRGGNixkmVzh3Ay8BIEVkMlALXuBpVmHq7Bv9EZQVZtPmVQw3N9MuPza8MY4zpjpOEvwE4DxgLCLCFOBuDp+JYjBN+8GLxwWNNlvCNMXHLSeJ+X1XbVHWDqq5X1VbgfbcDC8f+mkYyXJj4xKnQzV4HrB/fGBPHOm3hi0gZMAjIFpEpBFr3AAVATi/E5lhFTRMDC7PweKI78YlTZcGEbxdujTHxrKsunYuBrwLlwIPt1tcC33UxprDFsiQToG9OBhlej7XwjTFxrdOEr6qPA4+LyNWq+nwvxhS2ippGZo0qidnxPR6hf2EmB214BWNMHHPShz9VRIpCCyLSR0R+4PQAIuIVkVUi8peIIuyG369kpXsZ2je2vUwDCrKthW+MiWtOEv6lqloTWlDVo8BlYRzjVmBTuIE55fEIi+48n+9cONqtQzhSVpjFQRtewRgTx5wkfK+IZIYWRCQbyOxi+4+JSDnwOeC3kYWXOAYUZnHgWBOqdvOVMSY+OanD/yPwloj8jsDdtl8DHne4/58BdwH5nW0gIvOB+QBDhgxxuNv4U1aYRUubn6PHW+kbo/JQY4zpSrctfFV9APghcCowHrg/uK5LIjIHqAoOr9zV/h9R1WmqOq20tNRh2PHnk1p8u3BrjIlPTlr4qOprwGth7nsW8AURuQzIAgpE5I+qemOY+0kIobttK2ubGD+wMMbRGGPMyTpt4YvIe8HnOhGpbfeoE5Ha7nasqveoarmqDgOuA/6erMkeAuPpgN1ta4yJX13V4Z8dfO60/918ojQ/E69HXLvb9sMdhxlYlM3gGJefGmMSV1dDK/Tt6oOqesTpQVT1beBtx1ElIK9H6Jef6UoLf83eGm747YecNbKYJ+ZNj/r+jTGpoas+/BUEqnIEGAIcDb4uAvYAw12PLsG4MRFKQ3Mbt/1pNW1+ZfG2Q1TVNtGvwEbkNMaEr9M+fFUdrqojCExv+HlVLVHVYmAO8EJvBZhIArX40a3Suf8vG9l1uIEfXTkRv8LLayqiun9jTOpwcuPVGar6amghWLFznnshJa6y4PAK0br56vX1B3lm2V6+dd5I/mn6ECaXF/Li6v1R2bcxJvU4SfiHROTfRGSYiAwVkX8FDrsdWCIaUJjF8RYfdc1tPd5XZW0Td7+wlomDCrl99hgArpgyiPX7a9laWdfj/RtjUo+ThH89gWkNFwQfpcF15gTRGhff71fu/PMamlv9/Oy608hIC/wzzZk0EK9HWLDKWvnGmPA5udP2iKreCpyjqqer6m3hVOikkmjNfPXY4p38Y+shvjdnHCNL8z5eX5qfyTmjS3hpdYVNmG6MCVu3CV9EZorIRmBjcHmyiPzK9cgS0Cct/Mgv3G46UMsDr2/honH9uf7MwSe9f+WUQeyvaWTpLvvONcaEx0mXzk8JzH51GEBV1wDnuhlUouqXn4VI5C38plYftz6zisKcdH5y1URETp6y8bPjysjN8PKidesYY8LkJOGjqntPWOVzIZaEl5HmoSQvk8oIx8V/bPFOPqqs5z/nTqI4r+MRqLMzvFw8oYy/rjtAU6v9MxhjnHOS8PeKyExARSRDRO7ExQlNEl1ZQVZELfwjDS08vGg7s0/tx/lj+3W57ZVTBlHX1MaizVWRhmmMSUFOEv63gJuBQcA+4LTgsulApHfb/vzvW2loaeP/XHJKt9vOHFlCaX4mL1i3jjEmDF0OjywiXuBLqnpDL8WT8AYUZrF0Z3gXVHcfbuCPH+zmi2cMYXT/7seq83qEyycP5PH3d3G0oYU+NuGKMcaBLlv4quoDLu+lWJJCWWEWxxpbOd7i/OarB/62hXSvh9tnO5+X98rTB9HqU/667kAkYRpjUpCTCVAWi8gvgD8BDaGVqrrStagS2IB2N1+NaFdD35lVe47y17UHuPXC0WENijZuQAFj+ufx4qr93DhjaMTxtufzKwdrOx4aIiPNQ798G7TNmETmJOHPDD7f126dAhdEP5zEV1YQmPnKScJXVX786mZK8jL5xrkjwjqOiHDFlEE88PoW9hw+zpDiyMbJV1U2VNTy4qr9vLSmguq65k63fWLemZwzOnGnoTQm1XWb8FX1M70RSLII527bNzdWsnTXEX5wxQTyMh3NNvkpl58WSPgvrt7PLRc67w4CqKhp5MXV+3lx1X4+qqwn3SucP7Yf548tJd17Qk+fwr+/vJ63NlVZwjcmgXWbZUSkGPg+cDaBlv17wH2qagOodeDju227qcVv8/n5yeubGVGay3VnnHxHrRODirKZPrwv//POdl4JY9hknyo7DzWgClOH9uEHV0zgcxMHdHnx95W1FSzZfiiiOI0x8cFJs/IZ4F3g6uDyDQT682e7FVQiy0r30icnvdtx8f+0fC87qht45EtTSTuxRR2GOy8ey+8X70IJb2ydyycP4sopgxx3BZ01spgHXt9CdV0zpfkd3xRmjIlvThJ+X1W9v93yD0TkCrcCSgZlhdld1uLXN7fx0ze3cuawvlw0rn+PjnXGsL6cMazL2SijYtbIEmAL7+84zBcmD3T9eMaY6HPStFwkIteJiCf4uBb4q9uBJbIBhVlddun87r2dHKpv5p7LTulwvJx4NH5gAflZaSzZZt06xiQqJwn/m8BTQHPw8Qxwh4jUiUitm8Elqq7utvX5laeW7uHcMaVMGdKnlyOLXJrXw/ThxSzZbpdujElUTsbDz1dVj6qmBx+e4Lp8VS3ojSATTVlBFofqW2huO3lws3c+quLAsSauj/BCbSzNGlXMniPH2XvkeKxDMcZEIPKrhaZToUqdqtqTa9qfXrqXkrwMLjy1Z333sTBzZAkA71sr35iEZAnfBZ3V4lfVNvH3zVVcPbX842kLE8mY/nmU5GVYeaYxCcq1rCMiWSKyVETWiMgGEbnXrWPFm08S/qdLM/+8Yh8+v3LdGUNiEVaPiQhnjSxh8fbDHQ6/YIyJb44Svoh4RWSgiAwJPRx8rBm4QFUnExhS+RIRmdGTYBNFWeEnwyuE+P3KM8v2MGNEX4aX5MYqtB6bNbKY6rpmtlfXxzoUY0yYnMxp+x2gEniTQDnmX4G/dPc5DQhlhfTgIyWahXmZaeRnpn2qS2fJ9sPsPdLI9WcmZus+JNSPb9U6xiQeJy38W4GxqjpeVScGH5Oc7Dz4y2A1UAW8qaof9iTYRHJiaebTy/ZQlJPOxePLYhhVzw3um82gomwWWz2+MQnH0RSHwLFIdq6qPlU9DSgHzhSRCSduIyLzRWS5iCyvrq6O5DBxqawwiwPBm68O1zfzxoaDXDllEFnp3hhH1jMiwqxRxXyw4wg+f0r8YDMmaThJ+DuAt0XkHhG5I/QI5yCqWgO8DVzSwXuPqOo0VZ1WWpo8IzEOKMziYPCi7Qsr99Pq04TvzgmZObKEY42tbKyw++6MSSROEv4eAv33GUB+u0eXRKRURIqCr7MJDLa2OfJQE0tZYTbVdc20+vw8vWwPpw8pYoyD6QsTwcyRxQBWnmlMgnEyHv69ACKSq6oN3W3fzgDg8eC8uB7gWVXt9mJvshhQmIVf4dV1B9hR3cADcx1d9kgI/QqyGNUvjyXbD/PN80bGOhxjjENOqnTOEpGNwKbg8mQR+VV3n1PVtao6RVUnqeoEVb2vu88kk9Ddtg+9tZX8zDTmTBoQ44iia+bIYpbuPEJLmz/WoRhjHHLSpfMz4GLgMICqrgHOdTOoZFAWnJ92R3UDl08ZSE5G+DNaxbOZI0tobPWxZl9NrEMxxjjk6MYrVd17wqqTRwUznxK62xZI2DtruzJjRF9EYMk2q8c3JlE4KssUkZmAikiGiNxJsHvHdK4wO52sdA8TBxUyYVBhrMOJuqKcDCYMLGSxXbg1JmE46Wf4FvAQMAjYB7wB3OxmUMlARPjenHGcUpYclTkdmTmymMcW76SxxUd2RmLfX2BMKnAyHv4hVb1BVfuraj9VvdEmMHfmhulDmTrU/ekHY2XmqBJafcqyXUdiHYoxxgEnVTpjROQtEVkfXJ4kIv/mfmgm3p0xrA9pHrFuHWMShJM+/N8A9wCtECi3BK5zMyiTGHIy0phYXsiq3VapY0wicJLwc1R16Qnr2twIxiSeyeVFrK84ZuPqGJMAnCT8QyIykuDQxiIyFzjgalQmYUwqL+R4i49tVTY+vjHxzkmVzs3AI8ApIrIf2Anc4GpUJmFMKi8CYO2+GsYmcUWSMcnASZXODlWdDZQCp6jq2aq62/3QTCIYUZJLXmYaa/dFNIK2MaYXOanS2S4iTwJfAga7H5JJJB6PMGFQAWttiAVj4p6TPvxxwK+BYuC/RGSHiCxwNyyTSCaXF7HpQJ0NpGZMnHOS8H0ESjJ9gJ/A/LZVbgZlEsuk8iJafH62HKyLdSjGmC44uWhbC6wDHgR+Y3fZmhNNKg+MFbRmXw0Ty5Nv3CBjkoWTFv71wLvATcAzInKviFzoblgmkZT3yaZPTrr14xsT55zMePUS8JKInAJcCtwG3AVkuxybSRAiwqTyIqvUMSbOOanSeV5EthMYMTMX+DLQx+3ATGKZVF7I1qp6GltsqgRj4pWTPvyHgMWq+vH/ZBHJdC8kk4gmlRfh8ysbKo4xbVjyjhBqTCJzNMVh+2Qf9L4bwZjEFbpwa906xsSvTlv4IlJGYNKTbBGZAkjwrQIgpxdiMwmkf0EW/Qsy7cKtMXGsqy6di4GvAuUESjJDaoHvuhiTSVB24daY+NZpwlfVx4HHReRqVX2+F2MyCWpyeSFvbqyktqmVgqz0WIdjjDmBkz78xSLyqIi8BiAi40RknstxmQQUGjlzvbXyjYlLThL+74C/AQODyx8RqMU35lMmDgrdcWsJ35h45CThl6jqswTG0UFV2wiMq9MlERksIotEZJOIbBCRW3sYq4lzfXIzGNI3xy7cGhOnnNThN4hIMZ/MeDUDcNKEawP+RVVXikg+sEJE3lTVjZGHa+LdpPJCVu2xhG9MPHLSwr8DeBkYKSKLgT8A3+nuQ6p6QFVXBl/XAZsIlHmaJDapvJD9NY0crm+OdSjGmBM4GUtnpYicB4wlUIu/RVVbwzmIiAwDpgAfdvDefGA+wJAhQ8LZrYlDn0x5eIzPnNIvxtEYY9pzMpZOFnALcD9wL3BzcJ0jIpIHPA/cpqq1J76vqo+o6jRVnVZaWuo8chOXJgwqRMTuuDUmHjnpw/8DUAf8PLh8PfAEcE13HxSRdALJ/klVfSHSIE3iyMtMY2Rpnl24NSYOOUn4Y1V1crvlRSKyprsPiYgAjwKbVPXB7rY3yWNSeSHvfnQIVSXwZ2CMiQdOLtquClbmACAi04HFDj43i8DE5xeIyOrg47II4zQJZHJ5EYfqmzlY2xTrUIwx7XQ1eNo6AqWY6cCXRWRPcHko0G1ppaq+xycDrpkUEprmcM3eYwwotHlyjIkXXXXpzOm1KExSGTeggDSPsHZfDZdMKIt1OMaYoK4GT9vdm4GY5JGV7mVsWb5V6hgTZ5z04RsTtknlRazZV0NLmz/WoRhjgizhG1dcMqGMuqY2nluxL9ahGGOCHCV8ERkqIrODr7ODY+MY06lzR5cwZUgRv1y0zVr5xsQJJ3fafgN4Dvh1cFU58KKbQZnEJyLcNnsM+2sarZVvTJxw0sK/mUBNfS2Aqm4FbJAU0y1r5RsTX5wk/GZVbQktiEgawaGSjemKtfKNiS9OEv47IvJdIFtELgL+DLziblgmWVgr35j44STh3w1UA+uAbwKvAv/mZlAmeVgr35j44WTwtGzgMVX9DYCIeIPrjrsZmEke7Vv5c6eWk5EWf9XArT4/Ty/dw+H6lu43diAjzcO10wZTmp8Zlf0ZEw1OEv5bwGygPricDbwBzHQrKJNcQq38rzy2lOdW7OOfpsfXRDctbX5ueXoVr284GNX9/nXtAf78rbPIzXTy38wY9zn5S8xS1VCyR1XrRSTHxZhMEorXVn5zm4+bn1zJwk1V/PuccXzt7OFR2e/bW6r42u+Xceszq/n1l6bi9dg4gib2nPyvaxCR00MLIjIVaHQvJJOM4rEvv6nVxzefWMHCTVXcf/n4qCV7gPPH9uP/fmE8CzdV8uNXN0Vtv8b0hJMW/m3An0WkIrg8APiieyGZZNW+lT9lSBHpXg8ZXg9pXiHNK2R4PXg6aQlnpnnITPNGLZbGFh/zn1jOe9sO8eOrJnL9mdHvZvryWcPYUd3Ab9/bybCSXG6cMTQq+61vbmPVnqOs3F3DmP55XDpxQFT2a5Kfk0nMl4nIKXwyifnmcCcxNwY+3Zd/6UP/COuzGV4PZ40sZva4/lx0an/KCh1Pq3yS4y1tzPv9cj7YeZgHrp7ENdMGR7yv7nxvzjj2HDnO91/ewJC+OZw7Jvx5mw8ea+KDHYdZsfsoy3cfZcvBWvzBO2EyvB7GlOUzsjQvypGbZCSq3d9DJSIzgWG0+4JQ1T9EO5hp06bp8uXLo71bE2c+2HGYow0ttPqV1jY/bX4/Lb7Aa38nf48HjzWxcFMluw4HisMmlRcy+9T+XHBKP4rzMk7aXhV8fqW5zU9zm4+WNn/g4fPz87e2sXz3ER689jSumDLI1XOFQIt87sNL2H+0kRdumsno/s6Hovpgx2G+/OhSWnx+8jLTmDKkiNOH9GHasD4M7pPD5b9czNiyfJ75xoxOfx2Z5CYiK1R1mqNtu0v4IvIEMBJYDfiCq1VVb+lRlB2whG+6oqpsq6rnjY2VLNxUyao9kU2U7vUIP/viaXx+8sAoR9i5/TWNXPHLxWSmeXjx5lmU5HVfrrmjup4rf7WEkrwMHrpuCj7eB3wAAAxKSURBVKcOKDjp4u+zy/Zy1/Nr+Y+rJ/LFM+Kr+sn0jmgn/E3AOHXyU6CHLOGbcFTVNfH+9sM0tvg6fN/rETLSPB/3/2ekechI8zCgMIvyPr1faLZmbw1ffOR9hpfk8dhXp3U5/ePRhhauengJxxpbWXDTTIYW53a4napy/W8+YGNFLW/9y/lW95+Cop3w/wzcoqoHohFcVyzhm2T3j63V3PTHlWRnePnNl6cxeXDRSdu0tPm58dEPWb2nhqe+MZ1pw/p2uc8d1fVc8tA/uHh8GT+/fopboZs4FU7Cd1KWWQJsFJG/icjLoUfPQjQmNZ0zupTnb5oZuBP31+/z6rpPt6NUlXteWMfSnUf4z2smdZvsAUaU5vGdz4zilTUVLNpc5VboJgk4aeGf19F6VX0n2sFYC9+kisP1zcx/YgUrdh/lzs+O4ebPjEJE+OWibfzn37Zw++wx3Dp7tOP9tbT5+dz/+wfHW3y8cfu5dndvColql05vsoRvUklTq497XljHglX7uXLKIM4ZXcIdz67hyimDePDayYiEV3WzYvcRrn74feadPZzvzRnnUtQm3kS1S0dEZojIMhGpF5EWEfGJSG3PwzQmtWWle3nw2snc+dkxLFi1nzueXcMZw/rwk6snhp3sAaYO7cuNM4bwu8U7Wbsvsgomk9yc/O77BXAdgXHwpwFfBpz/1jTGdEpE+PYFoxlZmsdLqyv40VUTe3RH8V2XnMIbGyq57ZnVnDO65KT3PR7hqzOHdVr1Y5Kbo44+Vd0mIl5V9QG/E5El3X1GRB4D5gBVqjqhh3Eak9QunTggKkMkFGSl8x9zJ3HP8+t4aU3FSe/XNrZS29jGf187ucfHMonHScI/LiIZwGoReQA4ADhpHvyewK+DqN+Ra4zp3GfG9uOD717Y4Xt3P7+Wl9dUcN/l4+3CbgpyUpb5peB23wYagMHAVd19SFXfBY70KDpjTFTNnVrO8RbfSeWgJjU4SfhXqGqTqtaq6r2qegeBrpqoEJH5IrJcRJZXV1dHa7fGmA5MHdqHYcU5PL8yPoaoNr3LScL/SgfrvhqtAFT1EVWdpqrTSkvDH0nQGOOciDB3ajkf7DjC3iM2S2mq6TThi8j1IvIKMLz9HbYi8jZwuNciNMZE1ZWnlyOCtfJTUFdXbZYQuEBbAvx3u/V1wFo3gzLGuGdQUTYzRxbz/Mp93HLBaBtWOYV02sJX1d2q+jaBCcz/ERxK4QBQTmAilC6JyNPA+8BYEdknIvOiE7IxpqfmTi1n75FGlu2yuopU4qQu613gHBHpA7wFLCcwxeENXX1IVa/veXjGGDdcPL6MvMwNPLdiH9NHFPfKMV9ZU8G/LljHpPIizh5dwtmjShg3oMB+YfQiJwlfVPV4sIX+c1V9QERWuR2YMcY9ORlpXDaxjL+uPcC9l48nJ8PdmvyPKuu467m1DOqTTVVdEz95bTMAxbkZzBxVwtmjiulX0PG0lWP75zOwqPO5A4xzjhK+iJxFoEUf6paxOzaMSXBzpw7m2eX7eH39Qa46vdy149Q1tfKtJ1aQm5nGk1+fTv+CLCprm3hv6yHe2xZ4vNLBXcEh+VlpLLhpFqP62by9PeUkcd8G3AMsUNUNIjICWORuWMYYt50xrA9D+ubw3Ip9riV8VeWu59ay+8jxj5M9QP+CLK6eWs7VU8tRVbZXN1DX1HrS5xtbfNzyzCrmPb6MF2+aRZ/ck+cvNs51m/CDF2vfabe8A4j6fLbGmN4Vqsl/8M2P2Hf0uCvTPj763k5eW3+Qey49hRmdXCsQkS5b77/+0jSuf+QD/vnJFTwxbzrpXie3D5mOdFWH/7Pg8ysn1OHbjFfGJIkrpwwCYMHK/VHf99KdR/jxa5u5ZHwZ888dEfF+pg4NDBn9wY4j/PtLG4inOTwSTVct/CeCz//VG4EYY3rf4L45nDWimOdW7uPbF4yKaBz+jlTVNXHzUysZ0jeHB66Z1OP9XnV6OVur6nn47e2M6Z/H/5o1PCpxppqu6vBXBJ/fATYCG1X1ndCjtwI0xrhr7tRydh8+zvLdR6Oyv1afn28/tYq6plYevvF0CrLSo7Lf//3ZsVw0rj/3/2Ujb2+xuXsj0ekUhxL4Sv4+gVEyhcCXQxuB0sz73AjGpjg0pvc1NLdxxg8XMrQ4l3EDCjrcJt0rpHmFNI8n+NpDukdobvNT39zG8RZf8LmNqtpmtlbV89MvTubKKdG9GNzQ3Mbc/3mffUeOs+DmmYzqlx/V/SeiqMxpKyK3A5cB81V1Z3DdCOBh4HVV/WmU4v2YJXxjYuOhhVt5dvneTt9v9flp82vg2acfL2emecjNTCM300tuRho5GV5yM9M4b0wpXz8n8n77ruyvaeTyX7xHmsfD5MGFJ72vCi0+P02tPhpb/TS1+Ghq89HY4qPN33G+y8nw8p0LRnHttMFR69bqLdFK+KuAi1T10AnrS4E3VHVKjyM9gSV8YxKHqsYsOa7ac5T7/rKRxhZfh+9npHnISveSle4lO91DdvB1mleQDkaG2XiglhW7jzJzZDE/vmpiQk0BGa2Ev76zqQm7eq8nLOEbY2LB71eeWbaXH7+6iVa/nzsuGsPXZg0nLQFKQMNJ+F1V6bRE+J4xxiQUj0f4p+lDuOCUfnzvpfX86NXNvLLmAD+5eiLjBxbi8ytHGlqormumur6ZQ3XN1De3Re342elerj1jcNT215muWvg+AlManvQWkKWq0bn03o618I0xsaaqvLruIN9/eT1Hj7fSJyeDIw3NdNL9HxUleZks/7fZEX02Ki18VfVGdHRjjElgIsLnJg1g1qhifvX2duqaWinJy6Q0P5PS0HN+JnmZaVG7htFbV0JsEDRjjOlAUU4G373s1FiHEVXxf0XCGGNMVFjCN8aYFGEJ3xhjUoQlfGOMSRGW8I0xJkVYwjfGmBRhCd8YY1KEJXxjjEkRnQ6tEAsiUg3sjvDjJcChbrdKTql87pDa52/nnrpC5z9UVUudfCCuEn5PiMhyp+NJJJtUPndI7fO3c0/Nc4fIzt+6dIwxJkVYwjfGmBSRTAn/kVgHEEOpfO6Q2udv5566wj7/pOnDN8YY07VkauEbY4zpgiV8Y4xJEQmf8EXkEhHZIiLbROTuWMfjNhF5TESqRGR9u3V9ReRNEdkafO4TyxjdIiKDRWSRiGwSkQ0icmtwfaqcf5aILBWRNcHzvze4friIfBg8/z+JSEasY3WLiHhFZJWI/CW4nBLnLiK7RGSdiKwWkeXBdWH/3Sd0whcRL/BL4FJgHHC9iIyLbVSu+z1wyQnr7gbeUtXRwFvB5WTUBvyLqp4KzABuDv57p8r5NwMXqOpk4DTgEhGZAfwH8NPg+R8F5sUwRrfdCmxqt5xK5/4ZVT2tXe192H/3CZ3wgTOBbaq6Q1VbgGeAy2Mck6tU9V3gyAmrLwceD75+HLiiV4PqJap6QFVXBl/XEfiPP4jUOX9V1frgYnrwocAFwHPB9Ul7/iJSDnwO+G1wWUiRc+9E2H/3iZ7wBwF72y3vC65LNf1V9QAEkiLQL8bxuE5EhgFTgA9JofMPdmmsBqqAN4HtQI2qtgU3Seb/Az8D7gL8weViUufcFXhDRFaIyPzgurD/7hN9EvOOJnu3OtMkJyJ5wPPAbapaG2jopQZV9QGniUgRsADoaJbtpPs/ICJzgCpVXSEi54dWd7Bp0p170CxVrRCRfsCbIrI5kp0kegt/HzC43XI5UBGjWGKpUkQGAASfq2Icj2tEJJ1Asn9SVV8Irk6Z8w9R1RrgbQLXMopEJNR4S9b/A7OAL4jILgJdtxcQaPGnwrmjqhXB5yoCX/RnEsHffaIn/GXA6OCV+gzgOuDlGMcUCy8DXwm+/grwUgxjcU2wz/ZRYJOqPtjurVQ5/9Jgyx4RyQZmE7iOsQiYG9wsKc9fVe9R1XJVHUbg//nfVfUGUuDcRSRXRPJDr4HPAuuJ4O8+4e+0FZHLCHzTe4HHVPWHMQ7JVSLyNHA+gaFRK4HvAy8CzwJDgD3ANap64oXdhCciZwP/ANbxST/udwn046fC+U8icHHOS6Cx9qyq3iciIwi0evsCq4AbVbU5dpG6K9ilc6eqzkmFcw+e44LgYhrwlKr+UESKCfPvPuETvjHGGGcSvUvHGGOMQ5bwjTEmRVjCN8aYFGEJ3xhjUoQlfGOMSRGW8I0xJkVYwjfGmBTx/wFkz43ZgCC46AAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.plot(dists)\n",
    "plt.ylabel('Distance between predicted and actual')\n",
    "plt.xlabel('K')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "45"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Best K = min distance between predicted and actual\n",
    "best_K = dists.index(min(dists)) + 1\n",
    "best_K"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    Using the best value of k, demonstrate the functionality of your recommender by generating recommendations \n",
    "    for several anecdotal users (similar to part a, above)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "User 12 predicted 11.190444444444443 for joke 5.\n",
      "User 12 actual rating 6.0 for joke 5.\n"
     ]
    }
   ],
   "source": [
    "USER = 12\n",
    "JOKE = 5\n",
    "\n",
    "prediction = predictRating(USER, JOKE, best_K, item_sims)\n",
    "print(\"User {} predicted {} for joke {}.\".format(USER, prediction, JOKE))\n",
    "print(\"User {} actual rating {} for joke {}.\".format(USER, X_users[USER][JOKE], JOKE))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "User 89 predicted 9.6032 for joke 14.\n",
      "User 89 actual rating 12.12 for joke 14.\n"
     ]
    }
   ],
   "source": [
    "USER = 89\n",
    "JOKE = 14\n",
    "\n",
    "prediction = predictRating(USER, JOKE, best_K, item_sims)\n",
    "print(\"User {} predicted {} for joke {}.\".format(USER, prediction, JOKE))\n",
    "print(\"User {} actual rating {} for joke {}.\".format(USER, X_users[USER][JOKE], JOKE))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "User 94 predicted 11.222000000000001 for joke 76.\n",
      "User 94 actual rating 18.14 for joke 76.\n"
     ]
    }
   ],
   "source": [
    "USER = 94\n",
    "JOKE = 76\n",
    "\n",
    "prediction = predictRating(USER, JOKE, best_K, item_sims)\n",
    "print(\"User {} predicted {} for joke {}.\".format(USER, prediction, JOKE))\n",
    "print(\"User {} actual rating {} for joke {}.\".format(USER, X_users[USER][JOKE], JOKE))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
